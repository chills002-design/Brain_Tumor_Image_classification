{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a83b2d4a",
   "metadata": {},
   "source": [
    "BRAIN TUMOR MRI IMAGE CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9affb1",
   "metadata": {},
   "source": [
    "IMPORT THE IMAGE FILE SAVED IN LOCAL DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933e52d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059f5e19",
   "metadata": {},
   "source": [
    "CHECKING THE NUMBER OF IMAGES PER CLASS IN TRAIN, VALIDATION AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97360e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN set:\n",
      " - pituitary: 438 images\n",
      " - no_tumor: 335 images\n",
      " - glioma: 564 images\n",
      " - meningioma: 358 images\n",
      "\n",
      "VALID set:\n",
      " - pituitary: 118 images\n",
      " - no_tumor: 99 images\n",
      " - glioma: 161 images\n",
      " - meningioma: 124 images\n",
      "\n",
      "TEST set:\n",
      " - pituitary: 54 images\n",
      " - no_tumor: 49 images\n",
      " - glioma: 80 images\n",
      " - meningioma: 63 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = r\"/Users/srividyarajagopalan/Desktop/My Files/Data Science/DS_Projects/Brain_Tumor_Image_Class/Tumor\" # Path to the dataset\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    path = os.path.join(data_dir, split)\n",
    "    print(f\"\\n{split.upper()} set:\")\n",
    "    for cls in os.listdir(path):\n",
    "        cls_path = os.path.join(path, cls)\n",
    "        if os.path.isdir(cls_path):\n",
    "            print(f\" - {cls}: {len(os.listdir(cls_path))} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db9f1e1",
   "metadata": {},
   "source": [
    "DATA PROCESSING AND AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0779a5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['glioma', 'meningioma', 'no_tumor', 'pituitary']\n",
      "Train images: 1695\n",
      "Valid images: 502\n",
      "Test images: 246\n"
     ]
    }
   ],
   "source": [
    "## Data pre-processing and loading\n",
    "## Device configuration\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "data = r\"/Users/srividyarajagopalan/Desktop/My Files/Data Science/DS_Projects/Brain_Tumor_Image_Class/Tumor\"\n",
    "train_dir = os.path.join(data, 'train')\n",
    "test_dir = os.path.join(data, 'test')\n",
    "valid_dir = os.path.join(data, 'valid')\n",
    "\n",
    "\n",
    "## Hyper-parameters\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "\n",
    "## Load Image dataset\n",
    "batch_size=32\n",
    "train_transform = transforms.Compose(\n",
    "    [transforms.Resize((224,224)),\n",
    "     ##augmentation can be added here like RandomHorizontalFlip, RandomRotation etc \n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomRotation(10),\n",
    "     transforms.CenterCrop(224),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485,0.456,0.406), (0.229,0.224,0.225))])\n",
    "\n",
    "val_test_transform = transforms.Compose(\n",
    "    [transforms.Resize((224,224)), \n",
    "     transforms.CenterCrop(224),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485,0.456,0.406), (0.229,0.224,0.225))])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "valid_dataset = torchvision.datasets.ImageFolder(root=valid_dir, transform=val_test_transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=test_dir, transform=val_test_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size,shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Classes:\", train_dataset.classes)\n",
    "print(\"Train images:\", len(train_dataset))\n",
    "print(\"Valid images:\", len(valid_dataset))\n",
    "print(\"Test images:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310088bd",
   "metadata": {},
   "source": [
    "BUILD A CUSTOM CNN ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "609fcc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/5], Step [1/53, Loss: 1.4797]\n",
      "Epoch[1/5], Step [2/53, Loss: 2.5051]\n",
      "Epoch[1/5], Step [3/53, Loss: 5.3843]\n",
      "Epoch[1/5], Step [4/53, Loss: 3.2761]\n",
      "Epoch[1/5], Step [5/53, Loss: 4.5211]\n",
      "Epoch[1/5], Step [6/53, Loss: 2.4355]\n",
      "Epoch[1/5], Step [7/53, Loss: 2.4870]\n",
      "Epoch[1/5], Step [8/53, Loss: 1.9705]\n",
      "Epoch[1/5], Step [9/53, Loss: 3.6088]\n",
      "Epoch[1/5], Step [10/53, Loss: 2.6217]\n",
      "Epoch[1/5], Step [11/53, Loss: 2.6957]\n",
      "Epoch[1/5], Step [12/53, Loss: 2.1622]\n",
      "Epoch[1/5], Step [13/53, Loss: 1.2087]\n",
      "Epoch[1/5], Step [14/53, Loss: 0.9234]\n",
      "Epoch[1/5], Step [15/53, Loss: 1.4426]\n",
      "Epoch[1/5], Step [16/53, Loss: 1.9468]\n",
      "Epoch[1/5], Step [17/53, Loss: 1.8951]\n",
      "Epoch[1/5], Step [18/53, Loss: 2.1054]\n",
      "Epoch[1/5], Step [19/53, Loss: 1.5693]\n",
      "Epoch[1/5], Step [20/53, Loss: 1.1714]\n",
      "Epoch[1/5], Step [21/53, Loss: 0.4680]\n",
      "Epoch[1/5], Step [22/53, Loss: 0.8925]\n",
      "Epoch[1/5], Step [23/53, Loss: 1.6782]\n",
      "Epoch[1/5], Step [24/53, Loss: 1.6721]\n",
      "Epoch[1/5], Step [25/53, Loss: 1.3691]\n",
      "Epoch[1/5], Step [26/53, Loss: 1.1669]\n",
      "Epoch[1/5], Step [27/53, Loss: 0.7362]\n",
      "Epoch[1/5], Step [28/53, Loss: 1.6305]\n",
      "Epoch[1/5], Step [29/53, Loss: 0.8654]\n",
      "Epoch[1/5], Step [30/53, Loss: 1.2656]\n",
      "Epoch[1/5], Step [31/53, Loss: 0.8315]\n",
      "Epoch[1/5], Step [32/53, Loss: 0.6738]\n",
      "Epoch[1/5], Step [33/53, Loss: 0.8569]\n",
      "Epoch[1/5], Step [34/53, Loss: 1.1279]\n",
      "Epoch[1/5], Step [35/53, Loss: 0.7429]\n",
      "Epoch[1/5], Step [36/53, Loss: 0.6664]\n",
      "Epoch[1/5], Step [37/53, Loss: 0.5863]\n",
      "Epoch[1/5], Step [38/53, Loss: 1.0174]\n",
      "Epoch[1/5], Step [39/53, Loss: 0.5964]\n",
      "Epoch[1/5], Step [40/53, Loss: 0.9356]\n",
      "Epoch[1/5], Step [41/53, Loss: 0.7308]\n",
      "Epoch[1/5], Step [42/53, Loss: 0.6102]\n",
      "Epoch[1/5], Step [43/53, Loss: 0.6529]\n",
      "Epoch[1/5], Step [44/53, Loss: 0.6013]\n",
      "Epoch[1/5], Step [45/53, Loss: 0.6711]\n",
      "Epoch[1/5], Step [46/53, Loss: 0.6033]\n",
      "Epoch[1/5], Step [47/53, Loss: 0.8417]\n",
      "Epoch[1/5], Step [48/53, Loss: 0.9842]\n",
      "Epoch[1/5], Step [49/53, Loss: 0.9063]\n",
      "Epoch[1/5], Step [50/53, Loss: 0.7296]\n",
      "Epoch[1/5], Step [51/53, Loss: 0.7447]\n",
      "Epoch[1/5], Step [52/53, Loss: 0.6950]\n",
      "Epoch[1/5], Step [53/53, Loss: 0.8265]\n",
      "Epoch[2/5], Step [1/53, Loss: 0.5604]\n",
      "Epoch[2/5], Step [2/53, Loss: 0.4704]\n",
      "Epoch[2/5], Step [3/53, Loss: 0.6397]\n",
      "Epoch[2/5], Step [4/53, Loss: 0.7064]\n",
      "Epoch[2/5], Step [5/53, Loss: 0.6905]\n",
      "Epoch[2/5], Step [6/53, Loss: 0.8557]\n",
      "Epoch[2/5], Step [7/53, Loss: 0.5013]\n",
      "Epoch[2/5], Step [8/53, Loss: 0.7168]\n",
      "Epoch[2/5], Step [9/53, Loss: 0.5604]\n",
      "Epoch[2/5], Step [10/53, Loss: 0.5963]\n",
      "Epoch[2/5], Step [11/53, Loss: 0.6452]\n",
      "Epoch[2/5], Step [12/53, Loss: 0.6343]\n",
      "Epoch[2/5], Step [13/53, Loss: 0.5068]\n",
      "Epoch[2/5], Step [14/53, Loss: 0.5656]\n",
      "Epoch[2/5], Step [15/53, Loss: 0.7509]\n",
      "Epoch[2/5], Step [16/53, Loss: 0.6471]\n",
      "Epoch[2/5], Step [17/53, Loss: 0.4137]\n",
      "Epoch[2/5], Step [18/53, Loss: 0.7239]\n",
      "Epoch[2/5], Step [19/53, Loss: 0.5491]\n",
      "Epoch[2/5], Step [20/53, Loss: 0.6006]\n",
      "Epoch[2/5], Step [21/53, Loss: 0.8239]\n",
      "Epoch[2/5], Step [22/53, Loss: 0.3959]\n",
      "Epoch[2/5], Step [23/53, Loss: 0.5727]\n",
      "Epoch[2/5], Step [24/53, Loss: 0.6673]\n",
      "Epoch[2/5], Step [25/53, Loss: 0.6540]\n",
      "Epoch[2/5], Step [26/53, Loss: 0.8112]\n",
      "Epoch[2/5], Step [27/53, Loss: 0.4749]\n",
      "Epoch[2/5], Step [28/53, Loss: 0.6624]\n",
      "Epoch[2/5], Step [29/53, Loss: 0.5979]\n",
      "Epoch[2/5], Step [30/53, Loss: 0.9436]\n",
      "Epoch[2/5], Step [31/53, Loss: 0.4537]\n",
      "Epoch[2/5], Step [32/53, Loss: 0.6646]\n",
      "Epoch[2/5], Step [33/53, Loss: 0.5746]\n",
      "Epoch[2/5], Step [34/53, Loss: 0.7497]\n",
      "Epoch[2/5], Step [35/53, Loss: 0.6346]\n",
      "Epoch[2/5], Step [36/53, Loss: 0.6398]\n",
      "Epoch[2/5], Step [37/53, Loss: 0.3146]\n",
      "Epoch[2/5], Step [38/53, Loss: 0.4443]\n",
      "Epoch[2/5], Step [39/53, Loss: 0.4952]\n",
      "Epoch[2/5], Step [40/53, Loss: 0.5696]\n",
      "Epoch[2/5], Step [41/53, Loss: 0.4903]\n",
      "Epoch[2/5], Step [42/53, Loss: 0.4040]\n",
      "Epoch[2/5], Step [43/53, Loss: 0.5812]\n",
      "Epoch[2/5], Step [44/53, Loss: 0.6220]\n",
      "Epoch[2/5], Step [45/53, Loss: 0.4872]\n",
      "Epoch[2/5], Step [46/53, Loss: 0.5219]\n",
      "Epoch[2/5], Step [47/53, Loss: 0.5576]\n",
      "Epoch[2/5], Step [48/53, Loss: 0.6019]\n",
      "Epoch[2/5], Step [49/53, Loss: 0.7390]\n",
      "Epoch[2/5], Step [50/53, Loss: 0.6910]\n",
      "Epoch[2/5], Step [51/53, Loss: 0.5845]\n",
      "Epoch[2/5], Step [52/53, Loss: 0.4524]\n",
      "Epoch[2/5], Step [53/53, Loss: 0.9517]\n",
      "Epoch[3/5], Step [1/53, Loss: 0.3296]\n",
      "Epoch[3/5], Step [2/53, Loss: 0.6899]\n",
      "Epoch[3/5], Step [3/53, Loss: 0.7581]\n",
      "Epoch[3/5], Step [4/53, Loss: 0.7317]\n",
      "Epoch[3/5], Step [5/53, Loss: 0.5245]\n",
      "Epoch[3/5], Step [6/53, Loss: 0.5228]\n",
      "Epoch[3/5], Step [7/53, Loss: 0.6502]\n",
      "Epoch[3/5], Step [8/53, Loss: 0.5946]\n",
      "Epoch[3/5], Step [9/53, Loss: 0.6209]\n",
      "Epoch[3/5], Step [10/53, Loss: 0.6683]\n",
      "Epoch[3/5], Step [11/53, Loss: 0.5046]\n",
      "Epoch[3/5], Step [12/53, Loss: 0.5009]\n",
      "Epoch[3/5], Step [13/53, Loss: 0.5759]\n",
      "Epoch[3/5], Step [14/53, Loss: 0.4574]\n",
      "Epoch[3/5], Step [15/53, Loss: 0.8990]\n",
      "Epoch[3/5], Step [16/53, Loss: 0.5664]\n",
      "Epoch[3/5], Step [17/53, Loss: 0.5390]\n",
      "Epoch[3/5], Step [18/53, Loss: 0.6366]\n",
      "Epoch[3/5], Step [19/53, Loss: 0.3478]\n",
      "Epoch[3/5], Step [20/53, Loss: 0.5205]\n",
      "Epoch[3/5], Step [21/53, Loss: 0.5646]\n",
      "Epoch[3/5], Step [22/53, Loss: 0.4284]\n",
      "Epoch[3/5], Step [23/53, Loss: 0.6580]\n",
      "Epoch[3/5], Step [24/53, Loss: 0.4301]\n",
      "Epoch[3/5], Step [25/53, Loss: 0.4500]\n",
      "Epoch[3/5], Step [26/53, Loss: 0.6433]\n",
      "Epoch[3/5], Step [27/53, Loss: 0.8306]\n",
      "Epoch[3/5], Step [28/53, Loss: 0.2939]\n",
      "Epoch[3/5], Step [29/53, Loss: 0.3577]\n",
      "Epoch[3/5], Step [30/53, Loss: 0.5804]\n",
      "Epoch[3/5], Step [31/53, Loss: 0.3154]\n",
      "Epoch[3/5], Step [32/53, Loss: 0.4178]\n",
      "Epoch[3/5], Step [33/53, Loss: 0.6083]\n",
      "Epoch[3/5], Step [34/53, Loss: 0.4798]\n",
      "Epoch[3/5], Step [35/53, Loss: 0.4530]\n",
      "Epoch[3/5], Step [36/53, Loss: 0.5496]\n",
      "Epoch[3/5], Step [37/53, Loss: 0.4100]\n",
      "Epoch[3/5], Step [38/53, Loss: 0.5338]\n",
      "Epoch[3/5], Step [39/53, Loss: 0.3938]\n",
      "Epoch[3/5], Step [40/53, Loss: 0.5337]\n",
      "Epoch[3/5], Step [41/53, Loss: 0.7346]\n",
      "Epoch[3/5], Step [42/53, Loss: 0.7519]\n",
      "Epoch[3/5], Step [43/53, Loss: 0.3311]\n",
      "Epoch[3/5], Step [44/53, Loss: 0.3284]\n",
      "Epoch[3/5], Step [45/53, Loss: 0.5359]\n",
      "Epoch[3/5], Step [46/53, Loss: 0.3100]\n",
      "Epoch[3/5], Step [47/53, Loss: 0.6018]\n",
      "Epoch[3/5], Step [48/53, Loss: 0.4528]\n",
      "Epoch[3/5], Step [49/53, Loss: 0.5146]\n",
      "Epoch[3/5], Step [50/53, Loss: 0.4074]\n",
      "Epoch[3/5], Step [51/53, Loss: 0.3062]\n",
      "Epoch[3/5], Step [52/53, Loss: 0.5916]\n",
      "Epoch[3/5], Step [53/53, Loss: 0.6025]\n",
      "Epoch[4/5], Step [1/53, Loss: 0.3302]\n",
      "Epoch[4/5], Step [2/53, Loss: 0.4735]\n",
      "Epoch[4/5], Step [3/53, Loss: 0.6439]\n",
      "Epoch[4/5], Step [4/53, Loss: 0.8341]\n",
      "Epoch[4/5], Step [5/53, Loss: 0.6391]\n",
      "Epoch[4/5], Step [6/53, Loss: 0.5291]\n",
      "Epoch[4/5], Step [7/53, Loss: 0.9931]\n",
      "Epoch[4/5], Step [8/53, Loss: 0.7469]\n",
      "Epoch[4/5], Step [9/53, Loss: 0.6460]\n",
      "Epoch[4/5], Step [10/53, Loss: 0.5406]\n",
      "Epoch[4/5], Step [11/53, Loss: 0.2938]\n",
      "Epoch[4/5], Step [12/53, Loss: 0.4410]\n",
      "Epoch[4/5], Step [13/53, Loss: 0.5552]\n",
      "Epoch[4/5], Step [14/53, Loss: 0.4548]\n",
      "Epoch[4/5], Step [15/53, Loss: 0.4556]\n",
      "Epoch[4/5], Step [16/53, Loss: 0.5434]\n",
      "Epoch[4/5], Step [17/53, Loss: 0.4380]\n",
      "Epoch[4/5], Step [18/53, Loss: 0.3356]\n",
      "Epoch[4/5], Step [19/53, Loss: 0.7050]\n",
      "Epoch[4/5], Step [20/53, Loss: 0.4446]\n",
      "Epoch[4/5], Step [21/53, Loss: 0.3679]\n",
      "Epoch[4/5], Step [22/53, Loss: 0.7000]\n",
      "Epoch[4/5], Step [23/53, Loss: 0.4363]\n",
      "Epoch[4/5], Step [24/53, Loss: 0.2121]\n",
      "Epoch[4/5], Step [25/53, Loss: 0.5437]\n",
      "Epoch[4/5], Step [26/53, Loss: 0.2239]\n",
      "Epoch[4/5], Step [27/53, Loss: 0.5250]\n",
      "Epoch[4/5], Step [28/53, Loss: 0.2730]\n",
      "Epoch[4/5], Step [29/53, Loss: 0.5053]\n",
      "Epoch[4/5], Step [30/53, Loss: 0.4172]\n",
      "Epoch[4/5], Step [31/53, Loss: 0.5135]\n",
      "Epoch[4/5], Step [32/53, Loss: 0.4004]\n",
      "Epoch[4/5], Step [33/53, Loss: 0.6049]\n",
      "Epoch[4/5], Step [34/53, Loss: 0.6535]\n",
      "Epoch[4/5], Step [35/53, Loss: 0.3132]\n",
      "Epoch[4/5], Step [36/53, Loss: 0.4233]\n",
      "Epoch[4/5], Step [37/53, Loss: 0.4572]\n",
      "Epoch[4/5], Step [38/53, Loss: 0.7420]\n",
      "Epoch[4/5], Step [39/53, Loss: 0.4385]\n",
      "Epoch[4/5], Step [40/53, Loss: 0.4973]\n",
      "Epoch[4/5], Step [41/53, Loss: 0.4790]\n",
      "Epoch[4/5], Step [42/53, Loss: 0.5827]\n",
      "Epoch[4/5], Step [43/53, Loss: 0.8115]\n",
      "Epoch[4/5], Step [44/53, Loss: 0.3454]\n",
      "Epoch[4/5], Step [45/53, Loss: 0.4760]\n",
      "Epoch[4/5], Step [46/53, Loss: 0.4686]\n",
      "Epoch[4/5], Step [47/53, Loss: 0.4060]\n",
      "Epoch[4/5], Step [48/53, Loss: 0.4227]\n",
      "Epoch[4/5], Step [49/53, Loss: 0.4092]\n",
      "Epoch[4/5], Step [50/53, Loss: 0.3527]\n",
      "Epoch[4/5], Step [51/53, Loss: 0.3996]\n",
      "Epoch[4/5], Step [52/53, Loss: 0.4133]\n",
      "Epoch[4/5], Step [53/53, Loss: 0.3261]\n",
      "Epoch[5/5], Step [1/53, Loss: 0.5093]\n",
      "Epoch[5/5], Step [2/53, Loss: 0.3529]\n",
      "Epoch[5/5], Step [3/53, Loss: 0.6406]\n",
      "Epoch[5/5], Step [4/53, Loss: 0.3258]\n",
      "Epoch[5/5], Step [5/53, Loss: 0.5259]\n",
      "Epoch[5/5], Step [6/53, Loss: 0.5585]\n",
      "Epoch[5/5], Step [7/53, Loss: 0.3126]\n",
      "Epoch[5/5], Step [8/53, Loss: 0.2984]\n",
      "Epoch[5/5], Step [9/53, Loss: 0.5609]\n",
      "Epoch[5/5], Step [10/53, Loss: 0.5350]\n",
      "Epoch[5/5], Step [11/53, Loss: 0.3874]\n",
      "Epoch[5/5], Step [12/53, Loss: 0.3942]\n",
      "Epoch[5/5], Step [13/53, Loss: 0.3002]\n",
      "Epoch[5/5], Step [14/53, Loss: 0.3353]\n",
      "Epoch[5/5], Step [15/53, Loss: 0.3394]\n",
      "Epoch[5/5], Step [16/53, Loss: 0.2541]\n",
      "Epoch[5/5], Step [17/53, Loss: 0.4909]\n",
      "Epoch[5/5], Step [18/53, Loss: 0.3027]\n",
      "Epoch[5/5], Step [19/53, Loss: 0.5690]\n",
      "Epoch[5/5], Step [20/53, Loss: 0.4129]\n",
      "Epoch[5/5], Step [21/53, Loss: 0.2914]\n",
      "Epoch[5/5], Step [22/53, Loss: 0.4401]\n",
      "Epoch[5/5], Step [23/53, Loss: 0.4756]\n",
      "Epoch[5/5], Step [24/53, Loss: 0.3973]\n",
      "Epoch[5/5], Step [25/53, Loss: 0.5192]\n",
      "Epoch[5/5], Step [26/53, Loss: 0.2555]\n",
      "Epoch[5/5], Step [27/53, Loss: 0.5439]\n",
      "Epoch[5/5], Step [28/53, Loss: 0.5442]\n",
      "Epoch[5/5], Step [29/53, Loss: 0.5300]\n",
      "Epoch[5/5], Step [30/53, Loss: 0.4549]\n",
      "Epoch[5/5], Step [31/53, Loss: 0.3674]\n",
      "Epoch[5/5], Step [32/53, Loss: 0.3520]\n",
      "Epoch[5/5], Step [33/53, Loss: 0.5987]\n",
      "Epoch[5/5], Step [34/53, Loss: 0.4156]\n",
      "Epoch[5/5], Step [35/53, Loss: 0.4329]\n",
      "Epoch[5/5], Step [36/53, Loss: 0.3338]\n",
      "Epoch[5/5], Step [37/53, Loss: 0.4044]\n",
      "Epoch[5/5], Step [38/53, Loss: 0.2053]\n",
      "Epoch[5/5], Step [39/53, Loss: 0.3618]\n",
      "Epoch[5/5], Step [40/53, Loss: 0.5208]\n",
      "Epoch[5/5], Step [41/53, Loss: 0.4429]\n",
      "Epoch[5/5], Step [42/53, Loss: 0.4761]\n",
      "Epoch[5/5], Step [43/53, Loss: 0.4820]\n",
      "Epoch[5/5], Step [44/53, Loss: 0.5706]\n",
      "Epoch[5/5], Step [45/53, Loss: 0.3945]\n",
      "Epoch[5/5], Step [46/53, Loss: 0.2990]\n",
      "Epoch[5/5], Step [47/53, Loss: 0.5207]\n",
      "Epoch[5/5], Step [48/53, Loss: 0.3313]\n",
      "Epoch[5/5], Step [49/53, Loss: 0.6246]\n",
      "Epoch[5/5], Step [50/53, Loss: 0.5899]\n",
      "Epoch[5/5], Step [51/53, Loss: 0.2693]\n",
      "Epoch[5/5], Step [52/53, Loss: 0.6902]\n",
      "Epoch[5/5], Step [53/53, Loss: 0.5446]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.88        80\n",
      "           1       0.78      0.68      0.73        63\n",
      "           2       0.77      0.90      0.83        49\n",
      "           3       0.91      0.98      0.95        54\n",
      "\n",
      "    accuracy                           0.85       246\n",
      "   macro avg       0.84      0.86      0.85       246\n",
      "weighted avg       0.85      0.85      0.85       246\n",
      "\n",
      "Confusion Matrix:\n",
      "[[69  9  1  1]\n",
      " [ 5 43 12  3]\n",
      " [ 1  3 44  1]\n",
      " [ 1  0  0 53]]\n"
     ]
    }
   ],
   "source": [
    "## Custom CNN Architecture\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,32, kernel_size=3, stride=1, padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32,64, kernel_size=3, stride=1, padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.batchnorm1(self.conv1(x))))  # -> [B, 32, H/2, W/2]\n",
    "        x = self.pool(F.relu(self.batchnorm2(self.conv2(x))))  # -> [B, 64, H/4, W/4]\n",
    "        x = self.pool(F.relu(self.batchnorm3(self.conv3(x))))  # -> [B, 128, H/8, W/8]\n",
    "\n",
    "        x = x.view(-1, 128 * 28 * 28)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "## Train the model    \n",
    "customcnn_model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(customcnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = customcnn_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'Epoch[{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}, Loss: {loss.item():.4f}]')\n",
    "\n",
    "\n",
    "## Testing the model\n",
    "customcnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = customcnn_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30479c07",
   "metadata": {},
   "source": [
    "BEST ACCURACY - CUSTOMCNN - MODEL SAVED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cba57e",
   "metadata": {},
   "source": [
    "TRANSFER LEARNING USING PRE-TRAINED MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38659021",
   "metadata": {},
   "source": [
    "a. ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b232e1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glioma', 'meningioma', 'no_tumor', 'pituitary']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check for classes in Train dataset\n",
    "train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b83c92e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/5], Step [1/53, Loss: 1.3488]\n",
      "Epoch[1/5], Step [2/53, Loss: 1.4135]\n",
      "Epoch[1/5], Step [3/53, Loss: 1.5274]\n",
      "Epoch[1/5], Step [4/53, Loss: 1.5129]\n",
      "Epoch[1/5], Step [5/53, Loss: 1.4641]\n",
      "Epoch[1/5], Step [6/53, Loss: 1.3555]\n",
      "Epoch[1/5], Step [7/53, Loss: 1.3253]\n",
      "Epoch[1/5], Step [8/53, Loss: 1.3208]\n",
      "Epoch[1/5], Step [9/53, Loss: 1.3253]\n",
      "Epoch[1/5], Step [10/53, Loss: 1.3863]\n",
      "Epoch[1/5], Step [11/53, Loss: 1.4967]\n",
      "Epoch[1/5], Step [12/53, Loss: 1.3457]\n",
      "Epoch[1/5], Step [13/53, Loss: 1.3786]\n",
      "Epoch[1/5], Step [14/53, Loss: 1.4511]\n",
      "Epoch[1/5], Step [15/53, Loss: 1.2964]\n",
      "Epoch[1/5], Step [16/53, Loss: 1.2671]\n",
      "Epoch[1/5], Step [17/53, Loss: 1.3865]\n",
      "Epoch[1/5], Step [18/53, Loss: 1.3219]\n",
      "Epoch[1/5], Step [19/53, Loss: 1.3830]\n",
      "Epoch[1/5], Step [20/53, Loss: 1.3140]\n",
      "Epoch[1/5], Step [21/53, Loss: 1.3451]\n",
      "Epoch[1/5], Step [22/53, Loss: 1.2927]\n",
      "Epoch[1/5], Step [23/53, Loss: 1.1515]\n",
      "Epoch[1/5], Step [24/53, Loss: 1.2853]\n",
      "Epoch[1/5], Step [25/53, Loss: 1.2815]\n",
      "Epoch[1/5], Step [26/53, Loss: 1.2561]\n",
      "Epoch[1/5], Step [27/53, Loss: 1.3506]\n",
      "Epoch[1/5], Step [28/53, Loss: 1.3444]\n",
      "Epoch[1/5], Step [29/53, Loss: 1.3385]\n",
      "Epoch[1/5], Step [30/53, Loss: 1.1782]\n",
      "Epoch[1/5], Step [31/53, Loss: 1.2325]\n",
      "Epoch[1/5], Step [32/53, Loss: 1.2300]\n",
      "Epoch[1/5], Step [33/53, Loss: 1.1605]\n",
      "Epoch[1/5], Step [34/53, Loss: 1.2977]\n",
      "Epoch[1/5], Step [35/53, Loss: 1.3010]\n",
      "Epoch[1/5], Step [36/53, Loss: 1.1547]\n",
      "Epoch[1/5], Step [37/53, Loss: 1.2673]\n",
      "Epoch[1/5], Step [38/53, Loss: 1.2530]\n",
      "Epoch[1/5], Step [39/53, Loss: 1.2602]\n",
      "Epoch[1/5], Step [40/53, Loss: 1.1905]\n",
      "Epoch[1/5], Step [41/53, Loss: 1.2936]\n",
      "Epoch[1/5], Step [42/53, Loss: 1.2084]\n",
      "Epoch[1/5], Step [43/53, Loss: 1.1935]\n",
      "Epoch[1/5], Step [44/53, Loss: 1.2422]\n",
      "Epoch[1/5], Step [45/53, Loss: 1.2357]\n",
      "Epoch[1/5], Step [46/53, Loss: 1.2043]\n",
      "Epoch[1/5], Step [47/53, Loss: 1.1464]\n",
      "Epoch[1/5], Step [48/53, Loss: 1.2189]\n",
      "Epoch[1/5], Step [49/53, Loss: 1.1438]\n",
      "Epoch[1/5], Step [50/53, Loss: 1.2378]\n",
      "Epoch[1/5], Step [51/53, Loss: 1.1225]\n",
      "Epoch[1/5], Step [52/53, Loss: 1.1589]\n",
      "Epoch[1/5], Step [53/53, Loss: 1.2284]\n",
      "Epoch[2/5], Step [1/53, Loss: 1.2038]\n",
      "Epoch[2/5], Step [2/53, Loss: 1.0774]\n",
      "Epoch[2/5], Step [3/53, Loss: 1.1338]\n",
      "Epoch[2/5], Step [4/53, Loss: 1.1417]\n",
      "Epoch[2/5], Step [5/53, Loss: 1.2279]\n",
      "Epoch[2/5], Step [6/53, Loss: 1.2072]\n",
      "Epoch[2/5], Step [7/53, Loss: 1.1683]\n",
      "Epoch[2/5], Step [8/53, Loss: 1.1870]\n",
      "Epoch[2/5], Step [9/53, Loss: 1.1135]\n",
      "Epoch[2/5], Step [10/53, Loss: 1.2548]\n",
      "Epoch[2/5], Step [11/53, Loss: 1.1097]\n",
      "Epoch[2/5], Step [12/53, Loss: 1.0891]\n",
      "Epoch[2/5], Step [13/53, Loss: 1.0756]\n",
      "Epoch[2/5], Step [14/53, Loss: 1.1417]\n",
      "Epoch[2/5], Step [15/53, Loss: 1.0574]\n",
      "Epoch[2/5], Step [16/53, Loss: 1.1669]\n",
      "Epoch[2/5], Step [17/53, Loss: 1.0667]\n",
      "Epoch[2/5], Step [18/53, Loss: 1.1816]\n",
      "Epoch[2/5], Step [19/53, Loss: 1.0725]\n",
      "Epoch[2/5], Step [20/53, Loss: 1.1060]\n",
      "Epoch[2/5], Step [21/53, Loss: 1.1905]\n",
      "Epoch[2/5], Step [22/53, Loss: 1.0984]\n",
      "Epoch[2/5], Step [23/53, Loss: 1.0709]\n",
      "Epoch[2/5], Step [24/53, Loss: 1.0560]\n",
      "Epoch[2/5], Step [25/53, Loss: 1.0174]\n",
      "Epoch[2/5], Step [26/53, Loss: 1.0905]\n",
      "Epoch[2/5], Step [27/53, Loss: 1.0027]\n",
      "Epoch[2/5], Step [28/53, Loss: 0.9505]\n",
      "Epoch[2/5], Step [29/53, Loss: 1.0047]\n",
      "Epoch[2/5], Step [30/53, Loss: 0.9800]\n",
      "Epoch[2/5], Step [31/53, Loss: 1.1462]\n",
      "Epoch[2/5], Step [32/53, Loss: 0.9895]\n",
      "Epoch[2/5], Step [33/53, Loss: 1.0946]\n",
      "Epoch[2/5], Step [34/53, Loss: 1.0966]\n",
      "Epoch[2/5], Step [35/53, Loss: 1.0388]\n",
      "Epoch[2/5], Step [36/53, Loss: 1.0788]\n",
      "Epoch[2/5], Step [37/53, Loss: 0.9900]\n",
      "Epoch[2/5], Step [38/53, Loss: 1.0593]\n",
      "Epoch[2/5], Step [39/53, Loss: 1.1139]\n",
      "Epoch[2/5], Step [40/53, Loss: 1.0407]\n",
      "Epoch[2/5], Step [41/53, Loss: 0.9596]\n",
      "Epoch[2/5], Step [42/53, Loss: 1.0312]\n",
      "Epoch[2/5], Step [43/53, Loss: 1.0533]\n",
      "Epoch[2/5], Step [44/53, Loss: 1.0429]\n",
      "Epoch[2/5], Step [45/53, Loss: 0.9673]\n",
      "Epoch[2/5], Step [46/53, Loss: 1.0488]\n",
      "Epoch[2/5], Step [47/53, Loss: 0.9276]\n",
      "Epoch[2/5], Step [48/53, Loss: 0.9341]\n",
      "Epoch[2/5], Step [49/53, Loss: 1.0048]\n",
      "Epoch[2/5], Step [50/53, Loss: 0.9309]\n",
      "Epoch[2/5], Step [51/53, Loss: 0.9754]\n",
      "Epoch[2/5], Step [52/53, Loss: 0.9146]\n",
      "Epoch[2/5], Step [53/53, Loss: 0.9902]\n",
      "Epoch[3/5], Step [1/53, Loss: 0.9778]\n",
      "Epoch[3/5], Step [2/53, Loss: 1.0873]\n",
      "Epoch[3/5], Step [3/53, Loss: 0.9132]\n",
      "Epoch[3/5], Step [4/53, Loss: 1.0196]\n",
      "Epoch[3/5], Step [5/53, Loss: 0.9322]\n",
      "Epoch[3/5], Step [6/53, Loss: 0.8239]\n",
      "Epoch[3/5], Step [7/53, Loss: 1.0540]\n",
      "Epoch[3/5], Step [8/53, Loss: 0.8964]\n",
      "Epoch[3/5], Step [9/53, Loss: 0.8618]\n",
      "Epoch[3/5], Step [10/53, Loss: 1.1298]\n",
      "Epoch[3/5], Step [11/53, Loss: 1.1098]\n",
      "Epoch[3/5], Step [12/53, Loss: 0.9346]\n",
      "Epoch[3/5], Step [13/53, Loss: 0.9177]\n",
      "Epoch[3/5], Step [14/53, Loss: 0.9801]\n",
      "Epoch[3/5], Step [15/53, Loss: 0.8905]\n",
      "Epoch[3/5], Step [16/53, Loss: 0.9654]\n",
      "Epoch[3/5], Step [17/53, Loss: 0.9209]\n",
      "Epoch[3/5], Step [18/53, Loss: 0.9761]\n",
      "Epoch[3/5], Step [19/53, Loss: 0.9039]\n",
      "Epoch[3/5], Step [20/53, Loss: 0.8769]\n",
      "Epoch[3/5], Step [21/53, Loss: 1.0186]\n",
      "Epoch[3/5], Step [22/53, Loss: 0.8728]\n",
      "Epoch[3/5], Step [23/53, Loss: 0.9892]\n",
      "Epoch[3/5], Step [24/53, Loss: 0.8856]\n",
      "Epoch[3/5], Step [25/53, Loss: 0.8050]\n",
      "Epoch[3/5], Step [26/53, Loss: 1.0328]\n",
      "Epoch[3/5], Step [27/53, Loss: 0.9473]\n",
      "Epoch[3/5], Step [28/53, Loss: 0.8222]\n",
      "Epoch[3/5], Step [29/53, Loss: 0.9209]\n",
      "Epoch[3/5], Step [30/53, Loss: 0.8727]\n",
      "Epoch[3/5], Step [31/53, Loss: 0.9039]\n",
      "Epoch[3/5], Step [32/53, Loss: 0.8996]\n",
      "Epoch[3/5], Step [33/53, Loss: 0.8972]\n",
      "Epoch[3/5], Step [34/53, Loss: 0.8776]\n",
      "Epoch[3/5], Step [35/53, Loss: 0.9588]\n",
      "Epoch[3/5], Step [36/53, Loss: 0.8967]\n",
      "Epoch[3/5], Step [37/53, Loss: 0.8160]\n",
      "Epoch[3/5], Step [38/53, Loss: 0.9661]\n",
      "Epoch[3/5], Step [39/53, Loss: 0.8294]\n",
      "Epoch[3/5], Step [40/53, Loss: 0.8799]\n",
      "Epoch[3/5], Step [41/53, Loss: 0.8741]\n",
      "Epoch[3/5], Step [42/53, Loss: 0.7988]\n",
      "Epoch[3/5], Step [43/53, Loss: 0.9542]\n",
      "Epoch[3/5], Step [44/53, Loss: 1.0025]\n",
      "Epoch[3/5], Step [45/53, Loss: 0.9314]\n",
      "Epoch[3/5], Step [46/53, Loss: 0.8625]\n",
      "Epoch[3/5], Step [47/53, Loss: 1.0650]\n",
      "Epoch[3/5], Step [48/53, Loss: 1.0749]\n",
      "Epoch[3/5], Step [49/53, Loss: 0.8487]\n",
      "Epoch[3/5], Step [50/53, Loss: 0.8306]\n",
      "Epoch[3/5], Step [51/53, Loss: 0.8965]\n",
      "Epoch[3/5], Step [52/53, Loss: 0.8700]\n",
      "Epoch[3/5], Step [53/53, Loss: 0.7843]\n",
      "Epoch[4/5], Step [1/53, Loss: 0.8833]\n",
      "Epoch[4/5], Step [2/53, Loss: 1.0189]\n",
      "Epoch[4/5], Step [3/53, Loss: 0.8745]\n",
      "Epoch[4/5], Step [4/53, Loss: 0.8579]\n",
      "Epoch[4/5], Step [5/53, Loss: 0.8077]\n",
      "Epoch[4/5], Step [6/53, Loss: 0.9544]\n",
      "Epoch[4/5], Step [7/53, Loss: 0.7865]\n",
      "Epoch[4/5], Step [8/53, Loss: 0.8542]\n",
      "Epoch[4/5], Step [9/53, Loss: 0.7747]\n",
      "Epoch[4/5], Step [10/53, Loss: 0.8838]\n",
      "Epoch[4/5], Step [11/53, Loss: 0.7339]\n",
      "Epoch[4/5], Step [12/53, Loss: 0.9969]\n",
      "Epoch[4/5], Step [13/53, Loss: 0.9644]\n",
      "Epoch[4/5], Step [14/53, Loss: 0.8147]\n",
      "Epoch[4/5], Step [15/53, Loss: 0.8404]\n",
      "Epoch[4/5], Step [16/53, Loss: 0.9083]\n",
      "Epoch[4/5], Step [17/53, Loss: 0.8156]\n",
      "Epoch[4/5], Step [18/53, Loss: 0.9334]\n",
      "Epoch[4/5], Step [19/53, Loss: 0.8742]\n",
      "Epoch[4/5], Step [20/53, Loss: 0.7985]\n",
      "Epoch[4/5], Step [21/53, Loss: 0.7771]\n",
      "Epoch[4/5], Step [22/53, Loss: 0.9516]\n",
      "Epoch[4/5], Step [23/53, Loss: 0.7474]\n",
      "Epoch[4/5], Step [24/53, Loss: 0.9209]\n",
      "Epoch[4/5], Step [25/53, Loss: 0.8774]\n",
      "Epoch[4/5], Step [26/53, Loss: 0.8143]\n",
      "Epoch[4/5], Step [27/53, Loss: 0.8347]\n",
      "Epoch[4/5], Step [28/53, Loss: 0.8530]\n",
      "Epoch[4/5], Step [29/53, Loss: 0.8531]\n",
      "Epoch[4/5], Step [30/53, Loss: 0.8898]\n",
      "Epoch[4/5], Step [31/53, Loss: 0.7704]\n",
      "Epoch[4/5], Step [32/53, Loss: 0.9129]\n",
      "Epoch[4/5], Step [33/53, Loss: 0.8262]\n",
      "Epoch[4/5], Step [34/53, Loss: 0.7706]\n",
      "Epoch[4/5], Step [35/53, Loss: 0.7329]\n",
      "Epoch[4/5], Step [36/53, Loss: 0.8381]\n",
      "Epoch[4/5], Step [37/53, Loss: 0.7254]\n",
      "Epoch[4/5], Step [38/53, Loss: 0.7601]\n",
      "Epoch[4/5], Step [39/53, Loss: 0.8083]\n",
      "Epoch[4/5], Step [40/53, Loss: 0.8045]\n",
      "Epoch[4/5], Step [41/53, Loss: 0.7068]\n",
      "Epoch[4/5], Step [42/53, Loss: 0.7101]\n",
      "Epoch[4/5], Step [43/53, Loss: 0.8292]\n",
      "Epoch[4/5], Step [44/53, Loss: 0.7429]\n",
      "Epoch[4/5], Step [45/53, Loss: 0.7020]\n",
      "Epoch[4/5], Step [46/53, Loss: 0.7077]\n",
      "Epoch[4/5], Step [47/53, Loss: 0.8394]\n",
      "Epoch[4/5], Step [48/53, Loss: 0.9763]\n",
      "Epoch[4/5], Step [49/53, Loss: 0.9605]\n",
      "Epoch[4/5], Step [50/53, Loss: 0.7733]\n",
      "Epoch[4/5], Step [51/53, Loss: 0.7999]\n",
      "Epoch[4/5], Step [52/53, Loss: 0.6680]\n",
      "Epoch[4/5], Step [53/53, Loss: 0.7673]\n",
      "Epoch[5/5], Step [1/53, Loss: 0.6949]\n",
      "Epoch[5/5], Step [2/53, Loss: 0.8283]\n",
      "Epoch[5/5], Step [3/53, Loss: 0.6286]\n",
      "Epoch[5/5], Step [4/53, Loss: 0.6716]\n",
      "Epoch[5/5], Step [5/53, Loss: 0.7146]\n",
      "Epoch[5/5], Step [6/53, Loss: 0.6914]\n",
      "Epoch[5/5], Step [7/53, Loss: 1.0143]\n",
      "Epoch[5/5], Step [8/53, Loss: 0.7856]\n",
      "Epoch[5/5], Step [9/53, Loss: 0.7979]\n",
      "Epoch[5/5], Step [10/53, Loss: 0.8593]\n",
      "Epoch[5/5], Step [11/53, Loss: 1.0214]\n",
      "Epoch[5/5], Step [12/53, Loss: 0.7466]\n",
      "Epoch[5/5], Step [13/53, Loss: 0.7221]\n",
      "Epoch[5/5], Step [14/53, Loss: 0.7542]\n",
      "Epoch[5/5], Step [15/53, Loss: 0.6449]\n",
      "Epoch[5/5], Step [16/53, Loss: 0.7282]\n",
      "Epoch[5/5], Step [17/53, Loss: 0.7469]\n",
      "Epoch[5/5], Step [18/53, Loss: 0.8802]\n",
      "Epoch[5/5], Step [19/53, Loss: 0.6902]\n",
      "Epoch[5/5], Step [20/53, Loss: 0.7608]\n",
      "Epoch[5/5], Step [21/53, Loss: 0.7878]\n",
      "Epoch[5/5], Step [22/53, Loss: 0.6822]\n",
      "Epoch[5/5], Step [23/53, Loss: 0.8528]\n",
      "Epoch[5/5], Step [24/53, Loss: 0.7874]\n",
      "Epoch[5/5], Step [25/53, Loss: 0.8456]\n",
      "Epoch[5/5], Step [26/53, Loss: 0.6588]\n",
      "Epoch[5/5], Step [27/53, Loss: 0.6892]\n",
      "Epoch[5/5], Step [28/53, Loss: 0.8501]\n",
      "Epoch[5/5], Step [29/53, Loss: 0.8053]\n",
      "Epoch[5/5], Step [30/53, Loss: 0.7415]\n",
      "Epoch[5/5], Step [31/53, Loss: 0.8541]\n",
      "Epoch[5/5], Step [32/53, Loss: 0.6202]\n",
      "Epoch[5/5], Step [33/53, Loss: 0.6111]\n",
      "Epoch[5/5], Step [34/53, Loss: 0.7331]\n",
      "Epoch[5/5], Step [35/53, Loss: 0.7979]\n",
      "Epoch[5/5], Step [36/53, Loss: 0.7586]\n",
      "Epoch[5/5], Step [37/53, Loss: 0.7159]\n",
      "Epoch[5/5], Step [38/53, Loss: 0.6261]\n",
      "Epoch[5/5], Step [39/53, Loss: 0.7452]\n",
      "Epoch[5/5], Step [40/53, Loss: 0.7501]\n",
      "Epoch[5/5], Step [41/53, Loss: 0.6763]\n",
      "Epoch[5/5], Step [42/53, Loss: 0.6695]\n",
      "Epoch[5/5], Step [43/53, Loss: 0.7621]\n",
      "Epoch[5/5], Step [44/53, Loss: 0.8705]\n",
      "Epoch[5/5], Step [45/53, Loss: 0.6078]\n",
      "Epoch[5/5], Step [46/53, Loss: 0.8911]\n",
      "Epoch[5/5], Step [47/53, Loss: 0.6673]\n",
      "Epoch[5/5], Step [48/53, Loss: 0.7290]\n",
      "Epoch[5/5], Step [49/53, Loss: 0.7454]\n",
      "Epoch[5/5], Step [50/53, Loss: 0.7976]\n",
      "Epoch[5/5], Step [51/53, Loss: 0.6360]\n",
      "Epoch[5/5], Step [52/53, Loss: 0.8955]\n",
      "Epoch[5/5], Step [53/53, Loss: 0.7054]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.95      0.84        80\n",
      "           1       0.83      0.62      0.71        63\n",
      "           2       0.90      0.78      0.84        49\n",
      "           3       0.88      0.93      0.90        54\n",
      "\n",
      "    accuracy                           0.83       246\n",
      "   macro avg       0.84      0.82      0.82       246\n",
      "weighted avg       0.83      0.83      0.82       246\n",
      "\n",
      "Confusion Matrix:\n",
      "[[76  4  0  0]\n",
      " [17 39  3  4]\n",
      " [ 6  2 38  3]\n",
      " [ 1  2  1 50]]\n"
     ]
    }
   ],
   "source": [
    "## ResNet50 model for BRAIN TUMOR dataset\n",
    "## Device configuration\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "## Hyper-parameters\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(f'Number of classes: {num_classes}')\n",
    "\n",
    "\n",
    "## Load the pretrained ResNet50 model\n",
    "resnet50_model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "## Freeze all the layers\n",
    "for param in resnet50_model.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "in_features = resnet50_model.fc.in_features\n",
    "\n",
    "## Modify the final layer to match the number of classes in Brain Tumor dataset\n",
    "resnet50_model.fc = nn.Linear(in_features, num_classes)\n",
    "model2 = resnet50_model.to(device)\n",
    "## Test the pretrained ResNet50 model\n",
    "## Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model2.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "## Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #Forward pass\n",
    "        outputs = model2(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        #backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f'Epoch[{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}, Loss: {loss.item():.4f}]')\n",
    "\n",
    "## Testing the ResNet50 model\n",
    "model2.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model2(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8bf0d3",
   "metadata": {},
   "source": [
    "b. MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c07fdbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/5], Step [1/53, Loss: 1.4370]\n",
      "Epoch[1/5], Step [2/53, Loss: 1.3574]\n",
      "Epoch[1/5], Step [3/53, Loss: 1.4768]\n",
      "Epoch[1/5], Step [4/53, Loss: 1.3649]\n",
      "Epoch[1/5], Step [5/53, Loss: 1.3492]\n",
      "Epoch[1/5], Step [6/53, Loss: 1.4321]\n",
      "Epoch[1/5], Step [7/53, Loss: 1.4060]\n",
      "Epoch[1/5], Step [8/53, Loss: 1.3042]\n",
      "Epoch[1/5], Step [9/53, Loss: 1.3730]\n",
      "Epoch[1/5], Step [10/53, Loss: 1.3824]\n",
      "Epoch[1/5], Step [11/53, Loss: 1.3115]\n",
      "Epoch[1/5], Step [12/53, Loss: 1.2921]\n",
      "Epoch[1/5], Step [13/53, Loss: 1.3811]\n",
      "Epoch[1/5], Step [14/53, Loss: 1.3488]\n",
      "Epoch[1/5], Step [15/53, Loss: 1.3613]\n",
      "Epoch[1/5], Step [16/53, Loss: 1.3753]\n",
      "Epoch[1/5], Step [17/53, Loss: 1.2835]\n",
      "Epoch[1/5], Step [18/53, Loss: 1.4210]\n",
      "Epoch[1/5], Step [19/53, Loss: 1.3022]\n",
      "Epoch[1/5], Step [20/53, Loss: 1.4087]\n",
      "Epoch[1/5], Step [21/53, Loss: 1.2114]\n",
      "Epoch[1/5], Step [22/53, Loss: 1.3125]\n",
      "Epoch[1/5], Step [23/53, Loss: 1.3097]\n",
      "Epoch[1/5], Step [24/53, Loss: 1.2817]\n",
      "Epoch[1/5], Step [25/53, Loss: 1.2811]\n",
      "Epoch[1/5], Step [26/53, Loss: 1.2672]\n",
      "Epoch[1/5], Step [27/53, Loss: 1.1878]\n",
      "Epoch[1/5], Step [28/53, Loss: 1.3139]\n",
      "Epoch[1/5], Step [29/53, Loss: 1.3364]\n",
      "Epoch[1/5], Step [30/53, Loss: 1.2531]\n",
      "Epoch[1/5], Step [31/53, Loss: 1.1496]\n",
      "Epoch[1/5], Step [32/53, Loss: 1.2691]\n",
      "Epoch[1/5], Step [33/53, Loss: 1.2154]\n",
      "Epoch[1/5], Step [34/53, Loss: 1.1798]\n",
      "Epoch[1/5], Step [35/53, Loss: 1.2468]\n",
      "Epoch[1/5], Step [36/53, Loss: 1.2873]\n",
      "Epoch[1/5], Step [37/53, Loss: 1.1351]\n",
      "Epoch[1/5], Step [38/53, Loss: 1.2460]\n",
      "Epoch[1/5], Step [39/53, Loss: 1.1997]\n",
      "Epoch[1/5], Step [40/53, Loss: 1.1061]\n",
      "Epoch[1/5], Step [41/53, Loss: 1.1111]\n",
      "Epoch[1/5], Step [42/53, Loss: 1.4148]\n",
      "Epoch[1/5], Step [43/53, Loss: 1.0786]\n",
      "Epoch[1/5], Step [44/53, Loss: 1.2707]\n",
      "Epoch[1/5], Step [45/53, Loss: 1.1962]\n",
      "Epoch[1/5], Step [46/53, Loss: 1.2405]\n",
      "Epoch[1/5], Step [47/53, Loss: 1.2055]\n",
      "Epoch[1/5], Step [48/53, Loss: 1.1959]\n",
      "Epoch[1/5], Step [49/53, Loss: 1.1989]\n",
      "Epoch[1/5], Step [50/53, Loss: 1.1724]\n",
      "Epoch[1/5], Step [51/53, Loss: 1.1213]\n",
      "Epoch[1/5], Step [52/53, Loss: 1.1692]\n",
      "Epoch[1/5], Step [53/53, Loss: 1.0796]\n",
      "Epoch[2/5], Step [1/53, Loss: 1.0699]\n",
      "Epoch[2/5], Step [2/53, Loss: 1.0991]\n",
      "Epoch[2/5], Step [3/53, Loss: 1.1670]\n",
      "Epoch[2/5], Step [4/53, Loss: 1.1817]\n",
      "Epoch[2/5], Step [5/53, Loss: 1.2205]\n",
      "Epoch[2/5], Step [6/53, Loss: 1.1270]\n",
      "Epoch[2/5], Step [7/53, Loss: 0.9191]\n",
      "Epoch[2/5], Step [8/53, Loss: 1.1898]\n",
      "Epoch[2/5], Step [9/53, Loss: 1.1069]\n",
      "Epoch[2/5], Step [10/53, Loss: 1.0437]\n",
      "Epoch[2/5], Step [11/53, Loss: 1.0654]\n",
      "Epoch[2/5], Step [12/53, Loss: 1.0607]\n",
      "Epoch[2/5], Step [13/53, Loss: 1.0603]\n",
      "Epoch[2/5], Step [14/53, Loss: 1.0335]\n",
      "Epoch[2/5], Step [15/53, Loss: 1.0738]\n",
      "Epoch[2/5], Step [16/53, Loss: 1.0884]\n",
      "Epoch[2/5], Step [17/53, Loss: 1.0440]\n",
      "Epoch[2/5], Step [18/53, Loss: 1.0740]\n",
      "Epoch[2/5], Step [19/53, Loss: 1.1046]\n",
      "Epoch[2/5], Step [20/53, Loss: 1.1164]\n",
      "Epoch[2/5], Step [21/53, Loss: 1.1219]\n",
      "Epoch[2/5], Step [22/53, Loss: 1.0423]\n",
      "Epoch[2/5], Step [23/53, Loss: 1.0810]\n",
      "Epoch[2/5], Step [24/53, Loss: 1.1371]\n",
      "Epoch[2/5], Step [25/53, Loss: 1.0571]\n",
      "Epoch[2/5], Step [26/53, Loss: 1.1449]\n",
      "Epoch[2/5], Step [27/53, Loss: 1.0681]\n",
      "Epoch[2/5], Step [28/53, Loss: 1.1158]\n",
      "Epoch[2/5], Step [29/53, Loss: 1.1465]\n",
      "Epoch[2/5], Step [30/53, Loss: 1.0484]\n",
      "Epoch[2/5], Step [31/53, Loss: 1.0832]\n",
      "Epoch[2/5], Step [32/53, Loss: 0.9917]\n",
      "Epoch[2/5], Step [33/53, Loss: 0.9879]\n",
      "Epoch[2/5], Step [34/53, Loss: 1.1207]\n",
      "Epoch[2/5], Step [35/53, Loss: 1.0398]\n",
      "Epoch[2/5], Step [36/53, Loss: 1.0319]\n",
      "Epoch[2/5], Step [37/53, Loss: 1.0003]\n",
      "Epoch[2/5], Step [38/53, Loss: 1.0413]\n",
      "Epoch[2/5], Step [39/53, Loss: 1.0189]\n",
      "Epoch[2/5], Step [40/53, Loss: 1.1167]\n",
      "Epoch[2/5], Step [41/53, Loss: 1.1417]\n",
      "Epoch[2/5], Step [42/53, Loss: 0.9290]\n",
      "Epoch[2/5], Step [43/53, Loss: 0.9974]\n",
      "Epoch[2/5], Step [44/53, Loss: 0.9765]\n",
      "Epoch[2/5], Step [45/53, Loss: 0.9511]\n",
      "Epoch[2/5], Step [46/53, Loss: 0.9850]\n",
      "Epoch[2/5], Step [47/53, Loss: 0.9691]\n",
      "Epoch[2/5], Step [48/53, Loss: 1.0165]\n",
      "Epoch[2/5], Step [49/53, Loss: 0.9865]\n",
      "Epoch[2/5], Step [50/53, Loss: 1.0528]\n",
      "Epoch[2/5], Step [51/53, Loss: 1.1228]\n",
      "Epoch[2/5], Step [52/53, Loss: 0.8746]\n",
      "Epoch[2/5], Step [53/53, Loss: 0.9419]\n",
      "Epoch[3/5], Step [1/53, Loss: 0.9027]\n",
      "Epoch[3/5], Step [2/53, Loss: 1.0104]\n",
      "Epoch[3/5], Step [3/53, Loss: 1.0602]\n",
      "Epoch[3/5], Step [4/53, Loss: 0.9911]\n",
      "Epoch[3/5], Step [5/53, Loss: 0.9426]\n",
      "Epoch[3/5], Step [6/53, Loss: 0.9382]\n",
      "Epoch[3/5], Step [7/53, Loss: 0.8540]\n",
      "Epoch[3/5], Step [8/53, Loss: 1.0468]\n",
      "Epoch[3/5], Step [9/53, Loss: 0.8010]\n",
      "Epoch[3/5], Step [10/53, Loss: 0.9516]\n",
      "Epoch[3/5], Step [11/53, Loss: 0.8994]\n",
      "Epoch[3/5], Step [12/53, Loss: 0.9458]\n",
      "Epoch[3/5], Step [13/53, Loss: 0.9531]\n",
      "Epoch[3/5], Step [14/53, Loss: 1.0463]\n",
      "Epoch[3/5], Step [15/53, Loss: 0.9634]\n",
      "Epoch[3/5], Step [16/53, Loss: 0.9453]\n",
      "Epoch[3/5], Step [17/53, Loss: 0.9962]\n",
      "Epoch[3/5], Step [18/53, Loss: 0.9794]\n",
      "Epoch[3/5], Step [19/53, Loss: 0.9784]\n",
      "Epoch[3/5], Step [20/53, Loss: 1.0291]\n",
      "Epoch[3/5], Step [21/53, Loss: 0.9918]\n",
      "Epoch[3/5], Step [22/53, Loss: 1.0210]\n",
      "Epoch[3/5], Step [23/53, Loss: 0.8664]\n",
      "Epoch[3/5], Step [24/53, Loss: 0.8915]\n",
      "Epoch[3/5], Step [25/53, Loss: 0.9527]\n",
      "Epoch[3/5], Step [26/53, Loss: 0.8882]\n",
      "Epoch[3/5], Step [27/53, Loss: 0.9319]\n",
      "Epoch[3/5], Step [28/53, Loss: 0.8792]\n",
      "Epoch[3/5], Step [29/53, Loss: 1.0421]\n",
      "Epoch[3/5], Step [30/53, Loss: 0.8768]\n",
      "Epoch[3/5], Step [31/53, Loss: 0.8804]\n",
      "Epoch[3/5], Step [32/53, Loss: 0.9626]\n",
      "Epoch[3/5], Step [33/53, Loss: 0.9738]\n",
      "Epoch[3/5], Step [34/53, Loss: 0.8140]\n",
      "Epoch[3/5], Step [35/53, Loss: 0.9688]\n",
      "Epoch[3/5], Step [36/53, Loss: 0.8260]\n",
      "Epoch[3/5], Step [37/53, Loss: 0.8278]\n",
      "Epoch[3/5], Step [38/53, Loss: 0.8308]\n",
      "Epoch[3/5], Step [39/53, Loss: 0.7129]\n",
      "Epoch[3/5], Step [40/53, Loss: 0.8244]\n",
      "Epoch[3/5], Step [41/53, Loss: 1.0037]\n",
      "Epoch[3/5], Step [42/53, Loss: 0.9930]\n",
      "Epoch[3/5], Step [43/53, Loss: 0.8816]\n",
      "Epoch[3/5], Step [44/53, Loss: 0.8137]\n",
      "Epoch[3/5], Step [45/53, Loss: 0.8340]\n",
      "Epoch[3/5], Step [46/53, Loss: 0.8270]\n",
      "Epoch[3/5], Step [47/53, Loss: 0.8297]\n",
      "Epoch[3/5], Step [48/53, Loss: 0.7586]\n",
      "Epoch[3/5], Step [49/53, Loss: 1.1036]\n",
      "Epoch[3/5], Step [50/53, Loss: 0.8989]\n",
      "Epoch[3/5], Step [51/53, Loss: 0.8853]\n",
      "Epoch[3/5], Step [52/53, Loss: 0.9357]\n",
      "Epoch[3/5], Step [53/53, Loss: 0.9088]\n",
      "Epoch[4/5], Step [1/53, Loss: 0.8849]\n",
      "Epoch[4/5], Step [2/53, Loss: 0.9856]\n",
      "Epoch[4/5], Step [3/53, Loss: 0.8231]\n",
      "Epoch[4/5], Step [4/53, Loss: 0.8308]\n",
      "Epoch[4/5], Step [5/53, Loss: 0.7601]\n",
      "Epoch[4/5], Step [6/53, Loss: 0.6910]\n",
      "Epoch[4/5], Step [7/53, Loss: 0.7979]\n",
      "Epoch[4/5], Step [8/53, Loss: 0.7941]\n",
      "Epoch[4/5], Step [9/53, Loss: 0.7941]\n",
      "Epoch[4/5], Step [10/53, Loss: 0.7171]\n",
      "Epoch[4/5], Step [11/53, Loss: 0.8108]\n",
      "Epoch[4/5], Step [12/53, Loss: 0.7775]\n",
      "Epoch[4/5], Step [13/53, Loss: 0.7032]\n",
      "Epoch[4/5], Step [14/53, Loss: 0.8077]\n",
      "Epoch[4/5], Step [15/53, Loss: 0.7992]\n",
      "Epoch[4/5], Step [16/53, Loss: 0.7029]\n",
      "Epoch[4/5], Step [17/53, Loss: 0.7913]\n",
      "Epoch[4/5], Step [18/53, Loss: 0.7850]\n",
      "Epoch[4/5], Step [19/53, Loss: 0.8816]\n",
      "Epoch[4/5], Step [20/53, Loss: 0.8306]\n",
      "Epoch[4/5], Step [21/53, Loss: 0.8512]\n",
      "Epoch[4/5], Step [22/53, Loss: 0.8262]\n",
      "Epoch[4/5], Step [23/53, Loss: 0.8230]\n",
      "Epoch[4/5], Step [24/53, Loss: 0.8108]\n",
      "Epoch[4/5], Step [25/53, Loss: 0.8162]\n",
      "Epoch[4/5], Step [26/53, Loss: 0.6871]\n",
      "Epoch[4/5], Step [27/53, Loss: 0.8979]\n",
      "Epoch[4/5], Step [28/53, Loss: 0.9573]\n",
      "Epoch[4/5], Step [29/53, Loss: 0.7419]\n",
      "Epoch[4/5], Step [30/53, Loss: 0.8843]\n",
      "Epoch[4/5], Step [31/53, Loss: 0.9410]\n",
      "Epoch[4/5], Step [32/53, Loss: 1.0606]\n",
      "Epoch[4/5], Step [33/53, Loss: 0.6889]\n",
      "Epoch[4/5], Step [34/53, Loss: 0.7141]\n",
      "Epoch[4/5], Step [35/53, Loss: 0.6700]\n",
      "Epoch[4/5], Step [36/53, Loss: 0.8994]\n",
      "Epoch[4/5], Step [37/53, Loss: 0.8525]\n",
      "Epoch[4/5], Step [38/53, Loss: 0.8229]\n",
      "Epoch[4/5], Step [39/53, Loss: 0.8674]\n",
      "Epoch[4/5], Step [40/53, Loss: 0.7404]\n",
      "Epoch[4/5], Step [41/53, Loss: 0.8812]\n",
      "Epoch[4/5], Step [42/53, Loss: 0.7637]\n",
      "Epoch[4/5], Step [43/53, Loss: 0.6724]\n",
      "Epoch[4/5], Step [44/53, Loss: 0.7927]\n",
      "Epoch[4/5], Step [45/53, Loss: 0.8059]\n",
      "Epoch[4/5], Step [46/53, Loss: 0.8404]\n",
      "Epoch[4/5], Step [47/53, Loss: 0.7987]\n",
      "Epoch[4/5], Step [48/53, Loss: 1.0773]\n",
      "Epoch[4/5], Step [49/53, Loss: 0.8202]\n",
      "Epoch[4/5], Step [50/53, Loss: 0.9319]\n",
      "Epoch[4/5], Step [51/53, Loss: 0.8398]\n",
      "Epoch[4/5], Step [52/53, Loss: 0.8591]\n",
      "Epoch[4/5], Step [53/53, Loss: 0.8645]\n",
      "Epoch[5/5], Step [1/53, Loss: 0.8631]\n",
      "Epoch[5/5], Step [2/53, Loss: 0.7860]\n",
      "Epoch[5/5], Step [3/53, Loss: 0.7499]\n",
      "Epoch[5/5], Step [4/53, Loss: 0.7785]\n",
      "Epoch[5/5], Step [5/53, Loss: 0.7277]\n",
      "Epoch[5/5], Step [6/53, Loss: 0.7832]\n",
      "Epoch[5/5], Step [7/53, Loss: 0.7298]\n",
      "Epoch[5/5], Step [8/53, Loss: 0.8444]\n",
      "Epoch[5/5], Step [9/53, Loss: 0.8430]\n",
      "Epoch[5/5], Step [10/53, Loss: 0.7104]\n",
      "Epoch[5/5], Step [11/53, Loss: 0.7657]\n",
      "Epoch[5/5], Step [12/53, Loss: 0.8278]\n",
      "Epoch[5/5], Step [13/53, Loss: 0.8355]\n",
      "Epoch[5/5], Step [14/53, Loss: 0.8612]\n",
      "Epoch[5/5], Step [15/53, Loss: 0.7914]\n",
      "Epoch[5/5], Step [16/53, Loss: 0.9149]\n",
      "Epoch[5/5], Step [17/53, Loss: 0.5566]\n",
      "Epoch[5/5], Step [18/53, Loss: 0.7154]\n",
      "Epoch[5/5], Step [19/53, Loss: 0.7980]\n",
      "Epoch[5/5], Step [20/53, Loss: 0.8292]\n",
      "Epoch[5/5], Step [21/53, Loss: 0.8006]\n",
      "Epoch[5/5], Step [22/53, Loss: 0.7346]\n",
      "Epoch[5/5], Step [23/53, Loss: 0.8144]\n",
      "Epoch[5/5], Step [24/53, Loss: 0.6850]\n",
      "Epoch[5/5], Step [25/53, Loss: 0.7768]\n",
      "Epoch[5/5], Step [26/53, Loss: 0.7676]\n",
      "Epoch[5/5], Step [27/53, Loss: 0.6752]\n",
      "Epoch[5/5], Step [28/53, Loss: 0.7356]\n",
      "Epoch[5/5], Step [29/53, Loss: 0.7403]\n",
      "Epoch[5/5], Step [30/53, Loss: 0.7173]\n",
      "Epoch[5/5], Step [31/53, Loss: 0.7712]\n",
      "Epoch[5/5], Step [32/53, Loss: 0.7912]\n",
      "Epoch[5/5], Step [33/53, Loss: 0.7002]\n",
      "Epoch[5/5], Step [34/53, Loss: 0.6920]\n",
      "Epoch[5/5], Step [35/53, Loss: 0.8968]\n",
      "Epoch[5/5], Step [36/53, Loss: 0.6825]\n",
      "Epoch[5/5], Step [37/53, Loss: 0.6674]\n",
      "Epoch[5/5], Step [38/53, Loss: 0.5500]\n",
      "Epoch[5/5], Step [39/53, Loss: 0.8125]\n",
      "Epoch[5/5], Step [40/53, Loss: 0.6987]\n",
      "Epoch[5/5], Step [41/53, Loss: 0.7326]\n",
      "Epoch[5/5], Step [42/53, Loss: 0.9126]\n",
      "Epoch[5/5], Step [43/53, Loss: 0.8259]\n",
      "Epoch[5/5], Step [44/53, Loss: 0.7531]\n",
      "Epoch[5/5], Step [45/53, Loss: 0.7447]\n",
      "Epoch[5/5], Step [46/53, Loss: 0.7157]\n",
      "Epoch[5/5], Step [47/53, Loss: 0.9132]\n",
      "Epoch[5/5], Step [48/53, Loss: 0.6436]\n",
      "Epoch[5/5], Step [49/53, Loss: 0.8621]\n",
      "Epoch[5/5], Step [50/53, Loss: 0.6693]\n",
      "Epoch[5/5], Step [51/53, Loss: 0.5653]\n",
      "Epoch[5/5], Step [52/53, Loss: 0.6680]\n",
      "Epoch[5/5], Step [53/53, Loss: 0.6757]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.94      0.88        80\n",
      "           1       0.82      0.44      0.58        63\n",
      "           2       0.83      0.80      0.81        49\n",
      "           3       0.69      0.94      0.80        54\n",
      "\n",
      "    accuracy                           0.78       246\n",
      "   macro avg       0.79      0.78      0.77       246\n",
      "weighted avg       0.80      0.78      0.77       246\n",
      "\n",
      "Confusion Matrix:\n",
      "[[75  3  0  2]\n",
      " [10 28  8 17]\n",
      " [ 5  1 39  4]\n",
      " [ 1  2  0 51]]\n"
     ]
    }
   ],
   "source": [
    "## MobileNet model for BRAIN TUMOR dataset\n",
    "## Device configuration\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "## Hyper-parameters\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(f'Number of classes: {num_classes}')\n",
    "\n",
    "## Load the pretrained MobileNet model\n",
    "mobilenetv2_model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "## Freeze all the layers\n",
    "for param in mobilenetv2_model.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "in_features = mobilenetv2_model.classifier[1].in_features\n",
    "\n",
    "## Modify the final layer to match the number of classes in Brain Tumor dataset\n",
    "mobilenetv2_model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "model3 = mobilenetv2_model.to(device)\n",
    "\n",
    "## Test the pretrained MobileNet model\n",
    "## Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model3.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "## Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #Forward pass\n",
    "        outputs = model3(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        #backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f'Epoch[{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}, Loss: {loss.item():.4f}]')\n",
    "\n",
    "## Testing the MobileNet model\n",
    "model3.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model3(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b8542",
   "metadata": {},
   "source": [
    "c. InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60b182da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['glioma', 'meningioma', 'no_tumor', 'pituitary']\n",
      "Train images: 1695\n",
      "Valid images: 502\n",
      "Test images: 246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/5], Step [1/53, Loss: 1.4817]\n",
      "Epoch[1/5], Step [2/53, Loss: 1.4305]\n",
      "Epoch[1/5], Step [3/53, Loss: 1.3461]\n",
      "Epoch[1/5], Step [4/53, Loss: 1.3930]\n",
      "Epoch[1/5], Step [5/53, Loss: 1.3496]\n",
      "Epoch[1/5], Step [6/53, Loss: 1.2784]\n",
      "Epoch[1/5], Step [7/53, Loss: 1.3173]\n",
      "Epoch[1/5], Step [8/53, Loss: 1.3759]\n",
      "Epoch[1/5], Step [9/53, Loss: 1.3834]\n",
      "Epoch[1/5], Step [10/53, Loss: 1.3469]\n",
      "Epoch[1/5], Step [11/53, Loss: 1.3326]\n",
      "Epoch[1/5], Step [12/53, Loss: 1.3870]\n",
      "Epoch[1/5], Step [13/53, Loss: 1.3951]\n",
      "Epoch[1/5], Step [14/53, Loss: 1.4083]\n",
      "Epoch[1/5], Step [15/53, Loss: 1.3906]\n",
      "Epoch[1/5], Step [16/53, Loss: 1.4027]\n",
      "Epoch[1/5], Step [17/53, Loss: 1.2473]\n",
      "Epoch[1/5], Step [18/53, Loss: 1.3139]\n",
      "Epoch[1/5], Step [19/53, Loss: 1.3078]\n",
      "Epoch[1/5], Step [20/53, Loss: 1.3876]\n",
      "Epoch[1/5], Step [21/53, Loss: 1.4019]\n",
      "Epoch[1/5], Step [22/53, Loss: 1.3667]\n",
      "Epoch[1/5], Step [23/53, Loss: 1.3134]\n",
      "Epoch[1/5], Step [24/53, Loss: 1.3300]\n",
      "Epoch[1/5], Step [25/53, Loss: 1.2425]\n",
      "Epoch[1/5], Step [26/53, Loss: 1.3375]\n",
      "Epoch[1/5], Step [27/53, Loss: 1.2575]\n",
      "Epoch[1/5], Step [28/53, Loss: 1.4141]\n",
      "Epoch[1/5], Step [29/53, Loss: 1.3827]\n",
      "Epoch[1/5], Step [30/53, Loss: 1.3584]\n",
      "Epoch[1/5], Step [31/53, Loss: 1.3168]\n",
      "Epoch[1/5], Step [32/53, Loss: 1.2775]\n",
      "Epoch[1/5], Step [33/53, Loss: 1.2284]\n",
      "Epoch[1/5], Step [34/53, Loss: 1.2089]\n",
      "Epoch[1/5], Step [35/53, Loss: 1.3502]\n",
      "Epoch[1/5], Step [36/53, Loss: 1.2302]\n",
      "Epoch[1/5], Step [37/53, Loss: 1.4156]\n",
      "Epoch[1/5], Step [38/53, Loss: 1.3285]\n",
      "Epoch[1/5], Step [39/53, Loss: 1.2576]\n",
      "Epoch[1/5], Step [40/53, Loss: 1.3190]\n",
      "Epoch[1/5], Step [41/53, Loss: 1.3253]\n",
      "Epoch[1/5], Step [42/53, Loss: 1.2829]\n",
      "Epoch[1/5], Step [43/53, Loss: 1.2512]\n",
      "Epoch[1/5], Step [44/53, Loss: 1.2548]\n",
      "Epoch[1/5], Step [45/53, Loss: 1.2625]\n",
      "Epoch[1/5], Step [46/53, Loss: 1.2617]\n",
      "Epoch[1/5], Step [47/53, Loss: 1.2525]\n",
      "Epoch[1/5], Step [48/53, Loss: 1.1944]\n",
      "Epoch[1/5], Step [49/53, Loss: 1.3333]\n",
      "Epoch[1/5], Step [50/53, Loss: 1.2055]\n",
      "Epoch[1/5], Step [51/53, Loss: 1.3029]\n",
      "Epoch[1/5], Step [52/53, Loss: 1.2122]\n",
      "Epoch[1/5], Step [53/53, Loss: 1.1746]\n",
      "Epoch[2/5], Step [1/53, Loss: 1.1723]\n",
      "Epoch[2/5], Step [2/53, Loss: 1.2096]\n",
      "Epoch[2/5], Step [3/53, Loss: 1.2849]\n",
      "Epoch[2/5], Step [4/53, Loss: 1.1343]\n",
      "Epoch[2/5], Step [5/53, Loss: 1.2521]\n",
      "Epoch[2/5], Step [6/53, Loss: 1.1938]\n",
      "Epoch[2/5], Step [7/53, Loss: 1.1511]\n",
      "Epoch[2/5], Step [8/53, Loss: 1.1858]\n",
      "Epoch[2/5], Step [9/53, Loss: 1.1886]\n",
      "Epoch[2/5], Step [10/53, Loss: 1.2363]\n",
      "Epoch[2/5], Step [11/53, Loss: 1.1883]\n",
      "Epoch[2/5], Step [12/53, Loss: 1.2368]\n",
      "Epoch[2/5], Step [13/53, Loss: 1.2499]\n",
      "Epoch[2/5], Step [14/53, Loss: 1.2232]\n",
      "Epoch[2/5], Step [15/53, Loss: 1.3273]\n",
      "Epoch[2/5], Step [16/53, Loss: 1.3042]\n",
      "Epoch[2/5], Step [17/53, Loss: 1.1580]\n",
      "Epoch[2/5], Step [18/53, Loss: 1.1392]\n",
      "Epoch[2/5], Step [19/53, Loss: 1.2603]\n",
      "Epoch[2/5], Step [20/53, Loss: 1.1266]\n",
      "Epoch[2/5], Step [21/53, Loss: 1.2442]\n",
      "Epoch[2/5], Step [22/53, Loss: 1.1798]\n",
      "Epoch[2/5], Step [23/53, Loss: 1.2351]\n",
      "Epoch[2/5], Step [24/53, Loss: 1.2029]\n",
      "Epoch[2/5], Step [25/53, Loss: 1.2750]\n",
      "Epoch[2/5], Step [26/53, Loss: 1.3284]\n",
      "Epoch[2/5], Step [27/53, Loss: 1.1321]\n",
      "Epoch[2/5], Step [28/53, Loss: 1.3702]\n",
      "Epoch[2/5], Step [29/53, Loss: 1.1851]\n",
      "Epoch[2/5], Step [30/53, Loss: 1.1791]\n",
      "Epoch[2/5], Step [31/53, Loss: 1.2211]\n",
      "Epoch[2/5], Step [32/53, Loss: 1.1569]\n",
      "Epoch[2/5], Step [33/53, Loss: 1.2487]\n",
      "Epoch[2/5], Step [34/53, Loss: 1.2347]\n",
      "Epoch[2/5], Step [35/53, Loss: 1.1823]\n",
      "Epoch[2/5], Step [36/53, Loss: 1.1958]\n",
      "Epoch[2/5], Step [37/53, Loss: 1.1655]\n",
      "Epoch[2/5], Step [38/53, Loss: 1.0789]\n",
      "Epoch[2/5], Step [39/53, Loss: 1.1999]\n",
      "Epoch[2/5], Step [40/53, Loss: 1.1350]\n",
      "Epoch[2/5], Step [41/53, Loss: 1.1270]\n",
      "Epoch[2/5], Step [42/53, Loss: 1.3613]\n",
      "Epoch[2/5], Step [43/53, Loss: 1.1449]\n",
      "Epoch[2/5], Step [44/53, Loss: 1.1524]\n",
      "Epoch[2/5], Step [45/53, Loss: 1.0929]\n",
      "Epoch[2/5], Step [46/53, Loss: 1.0729]\n",
      "Epoch[2/5], Step [47/53, Loss: 1.2343]\n",
      "Epoch[2/5], Step [48/53, Loss: 1.1607]\n",
      "Epoch[2/5], Step [49/53, Loss: 1.1564]\n",
      "Epoch[2/5], Step [50/53, Loss: 1.2797]\n",
      "Epoch[2/5], Step [51/53, Loss: 1.2049]\n",
      "Epoch[2/5], Step [52/53, Loss: 1.1846]\n",
      "Epoch[2/5], Step [53/53, Loss: 1.1720]\n",
      "Epoch[3/5], Step [1/53, Loss: 1.0880]\n",
      "Epoch[3/5], Step [2/53, Loss: 1.1628]\n",
      "Epoch[3/5], Step [3/53, Loss: 1.2579]\n",
      "Epoch[3/5], Step [4/53, Loss: 1.0730]\n",
      "Epoch[3/5], Step [5/53, Loss: 1.1275]\n",
      "Epoch[3/5], Step [6/53, Loss: 1.1477]\n",
      "Epoch[3/5], Step [7/53, Loss: 1.1932]\n",
      "Epoch[3/5], Step [8/53, Loss: 1.0679]\n",
      "Epoch[3/5], Step [9/53, Loss: 1.0937]\n",
      "Epoch[3/5], Step [10/53, Loss: 1.0998]\n",
      "Epoch[3/5], Step [11/53, Loss: 1.0529]\n",
      "Epoch[3/5], Step [12/53, Loss: 1.0214]\n",
      "Epoch[3/5], Step [13/53, Loss: 1.0738]\n",
      "Epoch[3/5], Step [14/53, Loss: 1.1467]\n",
      "Epoch[3/5], Step [15/53, Loss: 1.1110]\n",
      "Epoch[3/5], Step [16/53, Loss: 1.0963]\n",
      "Epoch[3/5], Step [17/53, Loss: 1.0547]\n",
      "Epoch[3/5], Step [18/53, Loss: 1.1239]\n",
      "Epoch[3/5], Step [19/53, Loss: 1.0882]\n",
      "Epoch[3/5], Step [20/53, Loss: 1.1613]\n",
      "Epoch[3/5], Step [21/53, Loss: 1.1925]\n",
      "Epoch[3/5], Step [22/53, Loss: 1.1740]\n",
      "Epoch[3/5], Step [23/53, Loss: 1.0794]\n",
      "Epoch[3/5], Step [24/53, Loss: 1.0096]\n",
      "Epoch[3/5], Step [25/53, Loss: 1.1592]\n",
      "Epoch[3/5], Step [26/53, Loss: 1.1205]\n",
      "Epoch[3/5], Step [27/53, Loss: 1.1204]\n",
      "Epoch[3/5], Step [28/53, Loss: 1.1490]\n",
      "Epoch[3/5], Step [29/53, Loss: 1.1163]\n",
      "Epoch[3/5], Step [30/53, Loss: 1.1023]\n",
      "Epoch[3/5], Step [31/53, Loss: 1.1300]\n",
      "Epoch[3/5], Step [32/53, Loss: 1.1487]\n",
      "Epoch[3/5], Step [33/53, Loss: 1.0386]\n",
      "Epoch[3/5], Step [34/53, Loss: 1.1358]\n",
      "Epoch[3/5], Step [35/53, Loss: 1.0288]\n",
      "Epoch[3/5], Step [36/53, Loss: 1.0044]\n",
      "Epoch[3/5], Step [37/53, Loss: 1.1658]\n",
      "Epoch[3/5], Step [38/53, Loss: 1.1052]\n",
      "Epoch[3/5], Step [39/53, Loss: 1.0621]\n",
      "Epoch[3/5], Step [40/53, Loss: 1.0558]\n",
      "Epoch[3/5], Step [41/53, Loss: 1.1037]\n",
      "Epoch[3/5], Step [42/53, Loss: 1.1471]\n",
      "Epoch[3/5], Step [43/53, Loss: 1.0573]\n",
      "Epoch[3/5], Step [44/53, Loss: 1.1619]\n",
      "Epoch[3/5], Step [45/53, Loss: 0.9842]\n",
      "Epoch[3/5], Step [46/53, Loss: 0.9884]\n",
      "Epoch[3/5], Step [47/53, Loss: 1.0660]\n",
      "Epoch[3/5], Step [48/53, Loss: 1.1245]\n",
      "Epoch[3/5], Step [49/53, Loss: 1.0630]\n",
      "Epoch[3/5], Step [50/53, Loss: 1.0638]\n",
      "Epoch[3/5], Step [51/53, Loss: 1.0505]\n",
      "Epoch[3/5], Step [52/53, Loss: 1.0108]\n",
      "Epoch[3/5], Step [53/53, Loss: 1.0408]\n",
      "Epoch[4/5], Step [1/53, Loss: 1.0961]\n",
      "Epoch[4/5], Step [2/53, Loss: 1.0500]\n",
      "Epoch[4/5], Step [3/53, Loss: 1.0030]\n",
      "Epoch[4/5], Step [4/53, Loss: 1.0747]\n",
      "Epoch[4/5], Step [5/53, Loss: 1.0324]\n",
      "Epoch[4/5], Step [6/53, Loss: 1.0162]\n",
      "Epoch[4/5], Step [7/53, Loss: 1.0225]\n",
      "Epoch[4/5], Step [8/53, Loss: 0.9978]\n",
      "Epoch[4/5], Step [9/53, Loss: 1.0064]\n",
      "Epoch[4/5], Step [10/53, Loss: 1.0668]\n",
      "Epoch[4/5], Step [11/53, Loss: 1.0924]\n",
      "Epoch[4/5], Step [12/53, Loss: 0.9585]\n",
      "Epoch[4/5], Step [13/53, Loss: 1.0194]\n",
      "Epoch[4/5], Step [14/53, Loss: 1.0843]\n",
      "Epoch[4/5], Step [15/53, Loss: 1.0435]\n",
      "Epoch[4/5], Step [16/53, Loss: 0.8550]\n",
      "Epoch[4/5], Step [17/53, Loss: 0.9688]\n",
      "Epoch[4/5], Step [18/53, Loss: 1.0011]\n",
      "Epoch[4/5], Step [19/53, Loss: 1.0556]\n",
      "Epoch[4/5], Step [20/53, Loss: 1.0739]\n",
      "Epoch[4/5], Step [21/53, Loss: 0.9274]\n",
      "Epoch[4/5], Step [22/53, Loss: 1.1116]\n",
      "Epoch[4/5], Step [23/53, Loss: 1.0139]\n",
      "Epoch[4/5], Step [24/53, Loss: 0.9771]\n",
      "Epoch[4/5], Step [25/53, Loss: 0.8813]\n",
      "Epoch[4/5], Step [26/53, Loss: 1.1115]\n",
      "Epoch[4/5], Step [27/53, Loss: 0.9574]\n",
      "Epoch[4/5], Step [28/53, Loss: 1.0889]\n",
      "Epoch[4/5], Step [29/53, Loss: 0.9807]\n",
      "Epoch[4/5], Step [30/53, Loss: 0.9438]\n",
      "Epoch[4/5], Step [31/53, Loss: 1.0571]\n",
      "Epoch[4/5], Step [32/53, Loss: 1.0424]\n",
      "Epoch[4/5], Step [33/53, Loss: 1.0070]\n",
      "Epoch[4/5], Step [34/53, Loss: 1.1106]\n",
      "Epoch[4/5], Step [35/53, Loss: 0.9450]\n",
      "Epoch[4/5], Step [36/53, Loss: 1.1241]\n",
      "Epoch[4/5], Step [37/53, Loss: 0.9595]\n",
      "Epoch[4/5], Step [38/53, Loss: 1.0500]\n",
      "Epoch[4/5], Step [39/53, Loss: 1.0749]\n",
      "Epoch[4/5], Step [40/53, Loss: 1.0708]\n",
      "Epoch[4/5], Step [41/53, Loss: 0.9451]\n",
      "Epoch[4/5], Step [42/53, Loss: 1.0326]\n",
      "Epoch[4/5], Step [43/53, Loss: 0.9538]\n",
      "Epoch[4/5], Step [44/53, Loss: 1.1326]\n",
      "Epoch[4/5], Step [45/53, Loss: 1.0562]\n",
      "Epoch[4/5], Step [46/53, Loss: 0.9835]\n",
      "Epoch[4/5], Step [47/53, Loss: 1.0285]\n",
      "Epoch[4/5], Step [48/53, Loss: 0.9474]\n",
      "Epoch[4/5], Step [49/53, Loss: 1.0573]\n",
      "Epoch[4/5], Step [50/53, Loss: 1.0010]\n",
      "Epoch[4/5], Step [51/53, Loss: 0.9628]\n",
      "Epoch[4/5], Step [52/53, Loss: 0.8779]\n",
      "Epoch[4/5], Step [53/53, Loss: 0.8729]\n",
      "Epoch[5/5], Step [1/53, Loss: 1.0475]\n",
      "Epoch[5/5], Step [2/53, Loss: 0.8736]\n",
      "Epoch[5/5], Step [3/53, Loss: 0.9583]\n",
      "Epoch[5/5], Step [4/53, Loss: 0.8870]\n",
      "Epoch[5/5], Step [5/53, Loss: 0.9756]\n",
      "Epoch[5/5], Step [6/53, Loss: 0.8449]\n",
      "Epoch[5/5], Step [7/53, Loss: 1.0742]\n",
      "Epoch[5/5], Step [8/53, Loss: 0.9583]\n",
      "Epoch[5/5], Step [9/53, Loss: 1.0438]\n",
      "Epoch[5/5], Step [10/53, Loss: 0.9449]\n",
      "Epoch[5/5], Step [11/53, Loss: 1.0443]\n",
      "Epoch[5/5], Step [12/53, Loss: 0.9316]\n",
      "Epoch[5/5], Step [13/53, Loss: 0.9945]\n",
      "Epoch[5/5], Step [14/53, Loss: 0.9242]\n",
      "Epoch[5/5], Step [15/53, Loss: 0.9307]\n",
      "Epoch[5/5], Step [16/53, Loss: 0.9631]\n",
      "Epoch[5/5], Step [17/53, Loss: 1.0778]\n",
      "Epoch[5/5], Step [18/53, Loss: 0.9884]\n",
      "Epoch[5/5], Step [19/53, Loss: 0.9413]\n",
      "Epoch[5/5], Step [20/53, Loss: 0.8513]\n",
      "Epoch[5/5], Step [21/53, Loss: 0.9862]\n",
      "Epoch[5/5], Step [22/53, Loss: 0.9829]\n",
      "Epoch[5/5], Step [23/53, Loss: 0.9535]\n",
      "Epoch[5/5], Step [24/53, Loss: 0.9568]\n",
      "Epoch[5/5], Step [25/53, Loss: 1.0091]\n",
      "Epoch[5/5], Step [26/53, Loss: 0.9225]\n",
      "Epoch[5/5], Step [27/53, Loss: 0.9183]\n",
      "Epoch[5/5], Step [28/53, Loss: 0.9144]\n",
      "Epoch[5/5], Step [29/53, Loss: 1.0456]\n",
      "Epoch[5/5], Step [30/53, Loss: 0.8428]\n",
      "Epoch[5/5], Step [31/53, Loss: 0.9195]\n",
      "Epoch[5/5], Step [32/53, Loss: 1.0601]\n",
      "Epoch[5/5], Step [33/53, Loss: 0.8828]\n",
      "Epoch[5/5], Step [34/53, Loss: 1.1220]\n",
      "Epoch[5/5], Step [35/53, Loss: 0.9710]\n",
      "Epoch[5/5], Step [36/53, Loss: 0.9763]\n",
      "Epoch[5/5], Step [37/53, Loss: 1.0090]\n",
      "Epoch[5/5], Step [38/53, Loss: 0.9468]\n",
      "Epoch[5/5], Step [39/53, Loss: 0.9505]\n",
      "Epoch[5/5], Step [40/53, Loss: 0.9495]\n",
      "Epoch[5/5], Step [41/53, Loss: 0.9347]\n",
      "Epoch[5/5], Step [42/53, Loss: 1.0085]\n",
      "Epoch[5/5], Step [43/53, Loss: 0.8889]\n",
      "Epoch[5/5], Step [44/53, Loss: 0.8634]\n",
      "Epoch[5/5], Step [45/53, Loss: 0.8363]\n",
      "Epoch[5/5], Step [46/53, Loss: 0.9581]\n",
      "Epoch[5/5], Step [47/53, Loss: 0.8406]\n",
      "Epoch[5/5], Step [48/53, Loss: 0.8694]\n",
      "Epoch[5/5], Step [49/53, Loss: 0.8958]\n",
      "Epoch[5/5], Step [50/53, Loss: 0.9531]\n",
      "Epoch[5/5], Step [51/53, Loss: 0.9333]\n",
      "Epoch[5/5], Step [52/53, Loss: 0.8818]\n",
      "Epoch[5/5], Step [53/53, Loss: 0.9549]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.93      0.80        80\n",
      "           1       0.80      0.38      0.52        63\n",
      "           2       0.80      0.76      0.78        49\n",
      "           3       0.77      0.94      0.85        54\n",
      "\n",
      "    accuracy                           0.76       246\n",
      "   macro avg       0.77      0.75      0.74       246\n",
      "weighted avg       0.77      0.76      0.74       246\n",
      "\n",
      "Confusion Matrix:\n",
      "[[74  3  0  3]\n",
      " [22 24  9  8]\n",
      " [ 6  2 37  4]\n",
      " [ 2  1  0 51]]\n"
     ]
    }
   ],
   "source": [
    "## InceptionV3 model for BRAIN TUMOR dataset\n",
    "## Device configuration\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "## Hyper-parameters\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "\n",
    "## Load Image dataset\n",
    "\n",
    "inceptionv3_train_transform = transforms.Compose(\n",
    "    [transforms.Resize((299,299)),\n",
    "     ##augmentation can be added here like RandomHorizontalFlip, RandomRotation etc \n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomRotation(10),\n",
    "     transforms.CenterCrop(299),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485,0.456,0.406), (0.229,0.224,0.225))])\n",
    "\n",
    "inceptionv3_val_test_transform = transforms.Compose(\n",
    "    [transforms.Resize((299,299)),\n",
    "     transforms.CenterCrop(299),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485,0.456,0.406), (0.229,0.224,0.225))])\n",
    "\n",
    "inception_train_dataset = torchvision.datasets.ImageFolder(root=train_dir, transform=inceptionv3_train_transform)\n",
    "inception_valid_dataset = torchvision.datasets.ImageFolder(root=valid_dir, transform=inceptionv3_val_test_transform)\n",
    "inception_test_dataset = torchvision.datasets.ImageFolder(root=test_dir, transform=inceptionv3_val_test_transform)\n",
    "\n",
    "inception_train_loader = torch.utils.data.DataLoader(inception_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "inception_valid_loader = torch.utils.data.DataLoader(inception_valid_dataset, batch_size=batch_size,shuffle=False)\n",
    "inception_test_loader = torch.utils.data.DataLoader(inception_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Classes:\", inception_train_dataset.classes)\n",
    "print(\"Train images:\", len(inception_train_dataset))\n",
    "print(\"Valid images:\", len(inception_valid_dataset))\n",
    "print(\"Test images:\", len(inception_test_dataset))\n",
    "\n",
    "inceptionv3_model = torchvision.models.inception_v3(pretrained=True)\n",
    "\n",
    "## Freeze all the layers\n",
    "for param in inceptionv3_model.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "in_features = inceptionv3_model.fc.in_features\n",
    "\n",
    "## Modify the final layer to match the number of classes in Brain Tumor dataset\n",
    "inceptionv3_model.fc = nn.Linear(in_features, num_classes)\n",
    "model4 = inceptionv3_model.to(device)\n",
    "\n",
    "## Test the pretrained InceptionV3 model\n",
    "## Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model4.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "## Train the model\n",
    "n_total_steps = len(inception_train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(inception_train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #Forward pass\n",
    "        outputs = model4(images)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        #backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f'Epoch[{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}, Loss: {loss.item():.4f}]')\n",
    "\n",
    "## Testing the InceptionV3 model\n",
    "model4.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in inception_test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model4(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())     \n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374280be",
   "metadata": {},
   "source": [
    "d. EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b876781b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/5], Step [1/53, Loss: 1.4159]\n",
      "Epoch[1/5], Step [2/53, Loss: 1.3651]\n",
      "Epoch[1/5], Step [3/53, Loss: 1.3808]\n",
      "Epoch[1/5], Step [4/53, Loss: 1.3665]\n",
      "Epoch[1/5], Step [5/53, Loss: 1.3511]\n",
      "Epoch[1/5], Step [6/53, Loss: 1.4325]\n",
      "Epoch[1/5], Step [7/53, Loss: 1.3087]\n",
      "Epoch[1/5], Step [8/53, Loss: 1.3147]\n",
      "Epoch[1/5], Step [9/53, Loss: 1.3982]\n",
      "Epoch[1/5], Step [10/53, Loss: 1.3846]\n",
      "Epoch[1/5], Step [11/53, Loss: 1.3101]\n",
      "Epoch[1/5], Step [12/53, Loss: 1.3347]\n",
      "Epoch[1/5], Step [13/53, Loss: 1.3503]\n",
      "Epoch[1/5], Step [14/53, Loss: 1.2969]\n",
      "Epoch[1/5], Step [15/53, Loss: 1.3424]\n",
      "Epoch[1/5], Step [16/53, Loss: 1.2846]\n",
      "Epoch[1/5], Step [17/53, Loss: 1.2637]\n",
      "Epoch[1/5], Step [18/53, Loss: 1.3278]\n",
      "Epoch[1/5], Step [19/53, Loss: 1.3400]\n",
      "Epoch[1/5], Step [20/53, Loss: 1.2824]\n",
      "Epoch[1/5], Step [21/53, Loss: 1.2847]\n",
      "Epoch[1/5], Step [22/53, Loss: 1.2678]\n",
      "Epoch[1/5], Step [23/53, Loss: 1.3040]\n",
      "Epoch[1/5], Step [24/53, Loss: 1.3202]\n",
      "Epoch[1/5], Step [25/53, Loss: 1.2538]\n",
      "Epoch[1/5], Step [26/53, Loss: 1.2277]\n",
      "Epoch[1/5], Step [27/53, Loss: 1.2742]\n",
      "Epoch[1/5], Step [28/53, Loss: 1.2705]\n",
      "Epoch[1/5], Step [29/53, Loss: 1.2268]\n",
      "Epoch[1/5], Step [30/53, Loss: 1.1975]\n",
      "Epoch[1/5], Step [31/53, Loss: 1.2506]\n",
      "Epoch[1/5], Step [32/53, Loss: 1.2930]\n",
      "Epoch[1/5], Step [33/53, Loss: 1.1666]\n",
      "Epoch[1/5], Step [34/53, Loss: 1.2161]\n",
      "Epoch[1/5], Step [35/53, Loss: 1.2639]\n",
      "Epoch[1/5], Step [36/53, Loss: 1.2276]\n",
      "Epoch[1/5], Step [37/53, Loss: 1.2376]\n",
      "Epoch[1/5], Step [38/53, Loss: 1.2131]\n",
      "Epoch[1/5], Step [39/53, Loss: 1.1908]\n",
      "Epoch[1/5], Step [40/53, Loss: 1.1832]\n",
      "Epoch[1/5], Step [41/53, Loss: 1.1965]\n",
      "Epoch[1/5], Step [42/53, Loss: 1.1781]\n",
      "Epoch[1/5], Step [43/53, Loss: 1.3439]\n",
      "Epoch[1/5], Step [44/53, Loss: 1.2407]\n",
      "Epoch[1/5], Step [45/53, Loss: 1.1953]\n",
      "Epoch[1/5], Step [46/53, Loss: 1.2170]\n",
      "Epoch[1/5], Step [47/53, Loss: 1.1650]\n",
      "Epoch[1/5], Step [48/53, Loss: 1.0955]\n",
      "Epoch[1/5], Step [49/53, Loss: 1.2224]\n",
      "Epoch[1/5], Step [50/53, Loss: 1.2070]\n",
      "Epoch[1/5], Step [51/53, Loss: 1.1548]\n",
      "Epoch[1/5], Step [52/53, Loss: 1.1462]\n",
      "Epoch[1/5], Step [53/53, Loss: 1.1505]\n",
      "Epoch[2/5], Step [1/53, Loss: 1.2629]\n",
      "Epoch[2/5], Step [2/53, Loss: 1.1395]\n",
      "Epoch[2/5], Step [3/53, Loss: 1.0812]\n",
      "Epoch[2/5], Step [4/53, Loss: 1.1327]\n",
      "Epoch[2/5], Step [5/53, Loss: 1.1159]\n",
      "Epoch[2/5], Step [6/53, Loss: 1.1768]\n",
      "Epoch[2/5], Step [7/53, Loss: 1.1058]\n",
      "Epoch[2/5], Step [8/53, Loss: 1.1303]\n",
      "Epoch[2/5], Step [9/53, Loss: 1.1689]\n",
      "Epoch[2/5], Step [10/53, Loss: 1.1120]\n",
      "Epoch[2/5], Step [11/53, Loss: 1.1507]\n",
      "Epoch[2/5], Step [12/53, Loss: 1.0822]\n",
      "Epoch[2/5], Step [13/53, Loss: 1.1401]\n",
      "Epoch[2/5], Step [14/53, Loss: 1.1481]\n",
      "Epoch[2/5], Step [15/53, Loss: 1.1575]\n",
      "Epoch[2/5], Step [16/53, Loss: 1.0600]\n",
      "Epoch[2/5], Step [17/53, Loss: 1.1869]\n",
      "Epoch[2/5], Step [18/53, Loss: 1.1469]\n",
      "Epoch[2/5], Step [19/53, Loss: 1.1159]\n",
      "Epoch[2/5], Step [20/53, Loss: 1.0746]\n",
      "Epoch[2/5], Step [21/53, Loss: 1.0483]\n",
      "Epoch[2/5], Step [22/53, Loss: 1.1705]\n",
      "Epoch[2/5], Step [23/53, Loss: 1.2504]\n",
      "Epoch[2/5], Step [24/53, Loss: 1.0231]\n",
      "Epoch[2/5], Step [25/53, Loss: 1.1108]\n",
      "Epoch[2/5], Step [26/53, Loss: 1.0592]\n",
      "Epoch[2/5], Step [27/53, Loss: 1.0976]\n",
      "Epoch[2/5], Step [28/53, Loss: 1.1772]\n",
      "Epoch[2/5], Step [29/53, Loss: 1.0869]\n",
      "Epoch[2/5], Step [30/53, Loss: 1.0530]\n",
      "Epoch[2/5], Step [31/53, Loss: 1.1651]\n",
      "Epoch[2/5], Step [32/53, Loss: 1.0732]\n",
      "Epoch[2/5], Step [33/53, Loss: 1.0147]\n",
      "Epoch[2/5], Step [34/53, Loss: 0.9712]\n",
      "Epoch[2/5], Step [35/53, Loss: 1.0462]\n",
      "Epoch[2/5], Step [36/53, Loss: 1.0269]\n",
      "Epoch[2/5], Step [37/53, Loss: 1.0797]\n",
      "Epoch[2/5], Step [38/53, Loss: 1.0149]\n",
      "Epoch[2/5], Step [39/53, Loss: 1.0373]\n",
      "Epoch[2/5], Step [40/53, Loss: 1.0388]\n",
      "Epoch[2/5], Step [41/53, Loss: 1.0777]\n",
      "Epoch[2/5], Step [42/53, Loss: 1.0148]\n",
      "Epoch[2/5], Step [43/53, Loss: 1.0813]\n",
      "Epoch[2/5], Step [44/53, Loss: 1.0391]\n",
      "Epoch[2/5], Step [45/53, Loss: 1.0388]\n",
      "Epoch[2/5], Step [46/53, Loss: 0.9526]\n",
      "Epoch[2/5], Step [47/53, Loss: 1.1463]\n",
      "Epoch[2/5], Step [48/53, Loss: 0.9650]\n",
      "Epoch[2/5], Step [49/53, Loss: 0.9940]\n",
      "Epoch[2/5], Step [50/53, Loss: 0.9491]\n",
      "Epoch[2/5], Step [51/53, Loss: 1.0421]\n",
      "Epoch[2/5], Step [52/53, Loss: 0.9910]\n",
      "Epoch[2/5], Step [53/53, Loss: 1.1312]\n",
      "Epoch[3/5], Step [1/53, Loss: 0.9931]\n",
      "Epoch[3/5], Step [2/53, Loss: 1.0435]\n",
      "Epoch[3/5], Step [3/53, Loss: 0.9835]\n",
      "Epoch[3/5], Step [4/53, Loss: 1.0395]\n",
      "Epoch[3/5], Step [5/53, Loss: 0.9425]\n",
      "Epoch[3/5], Step [6/53, Loss: 0.8502]\n",
      "Epoch[3/5], Step [7/53, Loss: 1.0051]\n",
      "Epoch[3/5], Step [8/53, Loss: 0.9609]\n",
      "Epoch[3/5], Step [9/53, Loss: 0.9705]\n",
      "Epoch[3/5], Step [10/53, Loss: 0.9378]\n",
      "Epoch[3/5], Step [11/53, Loss: 1.0859]\n",
      "Epoch[3/5], Step [12/53, Loss: 0.9372]\n",
      "Epoch[3/5], Step [13/53, Loss: 1.1849]\n",
      "Epoch[3/5], Step [14/53, Loss: 0.9713]\n",
      "Epoch[3/5], Step [15/53, Loss: 0.9800]\n",
      "Epoch[3/5], Step [16/53, Loss: 0.9757]\n",
      "Epoch[3/5], Step [17/53, Loss: 0.9955]\n",
      "Epoch[3/5], Step [18/53, Loss: 1.1202]\n",
      "Epoch[3/5], Step [19/53, Loss: 0.9756]\n",
      "Epoch[3/5], Step [20/53, Loss: 0.9703]\n",
      "Epoch[3/5], Step [21/53, Loss: 0.9197]\n",
      "Epoch[3/5], Step [22/53, Loss: 0.9232]\n",
      "Epoch[3/5], Step [23/53, Loss: 0.9641]\n",
      "Epoch[3/5], Step [24/53, Loss: 1.0733]\n",
      "Epoch[3/5], Step [25/53, Loss: 0.9734]\n",
      "Epoch[3/5], Step [26/53, Loss: 0.8359]\n",
      "Epoch[3/5], Step [27/53, Loss: 0.9915]\n",
      "Epoch[3/5], Step [28/53, Loss: 0.8445]\n",
      "Epoch[3/5], Step [29/53, Loss: 0.9333]\n",
      "Epoch[3/5], Step [30/53, Loss: 1.1021]\n",
      "Epoch[3/5], Step [31/53, Loss: 0.9048]\n",
      "Epoch[3/5], Step [32/53, Loss: 0.9737]\n",
      "Epoch[3/5], Step [33/53, Loss: 1.0141]\n",
      "Epoch[3/5], Step [34/53, Loss: 0.9620]\n",
      "Epoch[3/5], Step [35/53, Loss: 0.8866]\n",
      "Epoch[3/5], Step [36/53, Loss: 0.9575]\n",
      "Epoch[3/5], Step [37/53, Loss: 0.9804]\n",
      "Epoch[3/5], Step [38/53, Loss: 1.0222]\n",
      "Epoch[3/5], Step [39/53, Loss: 0.9811]\n",
      "Epoch[3/5], Step [40/53, Loss: 0.8453]\n",
      "Epoch[3/5], Step [41/53, Loss: 0.8133]\n",
      "Epoch[3/5], Step [42/53, Loss: 1.0338]\n",
      "Epoch[3/5], Step [43/53, Loss: 1.0312]\n",
      "Epoch[3/5], Step [44/53, Loss: 0.9337]\n",
      "Epoch[3/5], Step [45/53, Loss: 0.9659]\n",
      "Epoch[3/5], Step [46/53, Loss: 0.8779]\n",
      "Epoch[3/5], Step [47/53, Loss: 0.8257]\n",
      "Epoch[3/5], Step [48/53, Loss: 0.8616]\n",
      "Epoch[3/5], Step [49/53, Loss: 0.9283]\n",
      "Epoch[3/5], Step [50/53, Loss: 0.9675]\n",
      "Epoch[3/5], Step [51/53, Loss: 0.8594]\n",
      "Epoch[3/5], Step [52/53, Loss: 0.8492]\n",
      "Epoch[3/5], Step [53/53, Loss: 0.8984]\n",
      "Epoch[4/5], Step [1/53, Loss: 0.8811]\n",
      "Epoch[4/5], Step [2/53, Loss: 0.8907]\n",
      "Epoch[4/5], Step [3/53, Loss: 0.8871]\n",
      "Epoch[4/5], Step [4/53, Loss: 0.8636]\n",
      "Epoch[4/5], Step [5/53, Loss: 0.8320]\n",
      "Epoch[4/5], Step [6/53, Loss: 0.8583]\n",
      "Epoch[4/5], Step [7/53, Loss: 0.9195]\n",
      "Epoch[4/5], Step [8/53, Loss: 0.9192]\n",
      "Epoch[4/5], Step [9/53, Loss: 0.8815]\n",
      "Epoch[4/5], Step [10/53, Loss: 0.9378]\n",
      "Epoch[4/5], Step [11/53, Loss: 0.7894]\n",
      "Epoch[4/5], Step [12/53, Loss: 1.0201]\n",
      "Epoch[4/5], Step [13/53, Loss: 0.8676]\n",
      "Epoch[4/5], Step [14/53, Loss: 0.8860]\n",
      "Epoch[4/5], Step [15/53, Loss: 0.8575]\n",
      "Epoch[4/5], Step [16/53, Loss: 0.8602]\n",
      "Epoch[4/5], Step [17/53, Loss: 0.8946]\n",
      "Epoch[4/5], Step [18/53, Loss: 1.0175]\n",
      "Epoch[4/5], Step [19/53, Loss: 0.8178]\n",
      "Epoch[4/5], Step [20/53, Loss: 0.9780]\n",
      "Epoch[4/5], Step [21/53, Loss: 0.8726]\n",
      "Epoch[4/5], Step [22/53, Loss: 0.9899]\n",
      "Epoch[4/5], Step [23/53, Loss: 0.9268]\n",
      "Epoch[4/5], Step [24/53, Loss: 0.7971]\n",
      "Epoch[4/5], Step [25/53, Loss: 0.9029]\n",
      "Epoch[4/5], Step [26/53, Loss: 0.8457]\n",
      "Epoch[4/5], Step [27/53, Loss: 0.7094]\n",
      "Epoch[4/5], Step [28/53, Loss: 0.8487]\n",
      "Epoch[4/5], Step [29/53, Loss: 0.9695]\n",
      "Epoch[4/5], Step [30/53, Loss: 0.9028]\n",
      "Epoch[4/5], Step [31/53, Loss: 0.8691]\n",
      "Epoch[4/5], Step [32/53, Loss: 0.9651]\n",
      "Epoch[4/5], Step [33/53, Loss: 0.8306]\n",
      "Epoch[4/5], Step [34/53, Loss: 0.8100]\n",
      "Epoch[4/5], Step [35/53, Loss: 0.8690]\n",
      "Epoch[4/5], Step [36/53, Loss: 0.8288]\n",
      "Epoch[4/5], Step [37/53, Loss: 0.7947]\n",
      "Epoch[4/5], Step [38/53, Loss: 0.9503]\n",
      "Epoch[4/5], Step [39/53, Loss: 0.8190]\n",
      "Epoch[4/5], Step [40/53, Loss: 0.8328]\n",
      "Epoch[4/5], Step [41/53, Loss: 0.9552]\n",
      "Epoch[4/5], Step [42/53, Loss: 0.8546]\n",
      "Epoch[4/5], Step [43/53, Loss: 0.7767]\n",
      "Epoch[4/5], Step [44/53, Loss: 0.8783]\n",
      "Epoch[4/5], Step [45/53, Loss: 0.8016]\n",
      "Epoch[4/5], Step [46/53, Loss: 0.7813]\n",
      "Epoch[4/5], Step [47/53, Loss: 0.8253]\n",
      "Epoch[4/5], Step [48/53, Loss: 0.9838]\n",
      "Epoch[4/5], Step [49/53, Loss: 0.7432]\n",
      "Epoch[4/5], Step [50/53, Loss: 0.9123]\n",
      "Epoch[4/5], Step [51/53, Loss: 0.7668]\n",
      "Epoch[4/5], Step [52/53, Loss: 0.7900]\n",
      "Epoch[4/5], Step [53/53, Loss: 0.7434]\n",
      "Epoch[5/5], Step [1/53, Loss: 0.7471]\n",
      "Epoch[5/5], Step [2/53, Loss: 0.9039]\n",
      "Epoch[5/5], Step [3/53, Loss: 0.9223]\n",
      "Epoch[5/5], Step [4/53, Loss: 0.7985]\n",
      "Epoch[5/5], Step [5/53, Loss: 0.7321]\n",
      "Epoch[5/5], Step [6/53, Loss: 0.8125]\n",
      "Epoch[5/5], Step [7/53, Loss: 0.7895]\n",
      "Epoch[5/5], Step [8/53, Loss: 0.8179]\n",
      "Epoch[5/5], Step [9/53, Loss: 0.8043]\n",
      "Epoch[5/5], Step [10/53, Loss: 0.8680]\n",
      "Epoch[5/5], Step [11/53, Loss: 0.7621]\n",
      "Epoch[5/5], Step [12/53, Loss: 0.7514]\n",
      "Epoch[5/5], Step [13/53, Loss: 0.8487]\n",
      "Epoch[5/5], Step [14/53, Loss: 0.6867]\n",
      "Epoch[5/5], Step [15/53, Loss: 0.7851]\n",
      "Epoch[5/5], Step [16/53, Loss: 0.7920]\n",
      "Epoch[5/5], Step [17/53, Loss: 0.7308]\n",
      "Epoch[5/5], Step [18/53, Loss: 0.7757]\n",
      "Epoch[5/5], Step [19/53, Loss: 0.7087]\n",
      "Epoch[5/5], Step [20/53, Loss: 0.7315]\n",
      "Epoch[5/5], Step [21/53, Loss: 0.8891]\n",
      "Epoch[5/5], Step [22/53, Loss: 0.7595]\n",
      "Epoch[5/5], Step [23/53, Loss: 0.7047]\n",
      "Epoch[5/5], Step [24/53, Loss: 0.9032]\n",
      "Epoch[5/5], Step [25/53, Loss: 0.8381]\n",
      "Epoch[5/5], Step [26/53, Loss: 0.8499]\n",
      "Epoch[5/5], Step [27/53, Loss: 0.7419]\n",
      "Epoch[5/5], Step [28/53, Loss: 0.7956]\n",
      "Epoch[5/5], Step [29/53, Loss: 0.7888]\n",
      "Epoch[5/5], Step [30/53, Loss: 0.7988]\n",
      "Epoch[5/5], Step [31/53, Loss: 0.7618]\n",
      "Epoch[5/5], Step [32/53, Loss: 0.7648]\n",
      "Epoch[5/5], Step [33/53, Loss: 0.8261]\n",
      "Epoch[5/5], Step [34/53, Loss: 0.7479]\n",
      "Epoch[5/5], Step [35/53, Loss: 0.8393]\n",
      "Epoch[5/5], Step [36/53, Loss: 0.7991]\n",
      "Epoch[5/5], Step [37/53, Loss: 0.7912]\n",
      "Epoch[5/5], Step [38/53, Loss: 0.8449]\n",
      "Epoch[5/5], Step [39/53, Loss: 0.8499]\n",
      "Epoch[5/5], Step [40/53, Loss: 0.7082]\n",
      "Epoch[5/5], Step [41/53, Loss: 0.7901]\n",
      "Epoch[5/5], Step [42/53, Loss: 0.8271]\n",
      "Epoch[5/5], Step [43/53, Loss: 0.9183]\n",
      "Epoch[5/5], Step [44/53, Loss: 0.7543]\n",
      "Epoch[5/5], Step [45/53, Loss: 0.7981]\n",
      "Epoch[5/5], Step [46/53, Loss: 0.8805]\n",
      "Epoch[5/5], Step [47/53, Loss: 0.8361]\n",
      "Epoch[5/5], Step [48/53, Loss: 0.7414]\n",
      "Epoch[5/5], Step [49/53, Loss: 0.7697]\n",
      "Epoch[5/5], Step [50/53, Loss: 0.8477]\n",
      "Epoch[5/5], Step [51/53, Loss: 0.7208]\n",
      "Epoch[5/5], Step [52/53, Loss: 0.7489]\n",
      "Epoch[5/5], Step [53/53, Loss: 0.8396]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        80\n",
      "           1       0.83      0.54      0.65        63\n",
      "           2       0.77      0.90      0.83        49\n",
      "           3       0.78      0.98      0.87        54\n",
      "\n",
      "    accuracy                           0.83       246\n",
      "   macro avg       0.82      0.83      0.82       246\n",
      "weighted avg       0.83      0.83      0.82       246\n",
      "\n",
      "Confusion Matrix:\n",
      "[[73  5  0  2]\n",
      " [ 6 34 13 10]\n",
      " [ 1  1 44  3]\n",
      " [ 0  1  0 53]]\n"
     ]
    }
   ],
   "source": [
    "## EfficientNetB0 model for BRAIN TUMOR dataset\n",
    "## Device configuration\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "## Hyper-parameters\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "\n",
    "## Load the pretrained EfficientNetB0 model\n",
    "efficientb0_model = torchvision.models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "## Freeze all the layers\n",
    "for param in efficientb0_model.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "in_features = efficientb0_model.classifier[1].in_features\n",
    "\n",
    "## Modify the final layer to match the number of classes in Brain Tumor dataset\n",
    "efficientb0_model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "model5 = efficientb0_model.to(device)\n",
    "\n",
    "## Test the pretrained EfficientNetB0 model\n",
    "## Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model5.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "## Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #Forward pass\n",
    "        outputs = model5(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        #backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f'Epoch[{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}, Loss: {loss.item():.4f}]')\n",
    "\n",
    "## Testing the EfficientNetB0 model\n",
    "model5.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model5(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(labels.cpu().numpy())     \n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc652f01",
   "metadata": {},
   "source": [
    "COMPARE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3f39d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0kAAAIQCAYAAABUjyXLAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAARPZJREFUeJzt3Qm8VHX9P/4PyCoKCiqIguKSuGtagHtKErnhmmY/McmVXEuLcsNUlErMxPWrqClupZZ+UzPcsnDfN1wTFIFMASUBk/k/3p/ff+Z37uVeuODFuVyez8fjwL1nzsx8Zubcc87rfM7nPS1KpVIpAQAAkLX8v/8BAAAQhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAACoQkgCaoRYsW6cwzz1zk+/3zn//M973mmmuWSLugtp122ilPAM2JkARQjwgaEThieuSRR+a7vVQqpR49euTbd99997S0+vOf/5xfQ/fu3dO8efOq3ZylzsyZM9Pw4cPT5ptvnlZYYYXUvn37tMkmm6Sf/OQnafLkydVuHgCLodXi3AlgWdKuXbs0duzYtN1229WY/9BDD6V33303tW3bNi3NbrjhhrT22mvnXqj7778/9e/fv9pNWmq89dZb+f2aOHFi2n///dMRRxyR2rRpk55//vl01VVXpdtvvz299tprqTn7y1/+Uu0mADQ6PUkAC/Htb3873Xrrrem///1vjfkRnLbaaqvUrVu3tLSaNWtW+uMf/5hOOumktOWWW+bA1JTb2pTE+rDPPvukqVOnpgcffDDdeOONaejQoenwww9Pv/3tb3OAiuDUXP3nP//J/0cojAmgORGSABbioIMOSv/+97/TfffdV5k3d+7c9Pvf/z5997vfrfeA/kc/+lG+HC96mjbYYIP0q1/9Kl+iVzRnzpx04oknplVXXTWtuOKKac8998y9U3V577330mGHHZa6du2aH3PjjTdOV1999Rd6bdHT8emnn+aD+QMPPDDddtttafbs2fMtF/NijNRXvvKV3LO2+uqr54Dw5ptvVpaJS/V+85vfpE033TQvE6/pW9/6VnryyScXOl6q9his+Dnmvfzyy/k9XnnllSs9edFLc+ihh6Z11lknP0+E1Hhf4jOq6z0bMmRIvpQw3rNevXqlo48+On9+EWLiOUaNGjXf/f7xj3/k2yL41OcPf/hDeu6559LPf/7z+XoZQ8eOHdM555xTY16E7QjWcUneKquskr73ve/lNhbFa4vL9qJ3Ki7jjJ/XWGONNHr06Hz7Cy+8kHbeeefUoUOHtNZaa+WwXtdlog8//HA68sgjU5cuXXJbDjnkkPTRRx/VWDYC8m677VZ5f9Zdd930i1/8In3++ec1losxR3EJ4VNPPZV22GGHtPzyy6ef/exn9Y5JipAY62csF5/d1ltvPV87n3nmmTRw4MDctniNu+yyS3r00UfrfC1///vfc5CPdSpe9957753+9a9/1fvZAHxRQhLAQsSlaP369atxwHz33XenGTNm5GBRWwShCDtx8B0h4YILLsgh6eSTT84HekU/+MEP0oUXXph23XXXdN5556XWrVvng9baoreib9++6a9//Wv64Q9/mMPIeuutlwNA3H9xRc/RN77xjRw04rV8/PHH6c4776yxTBwwx8F6jLuJA/xf//rX6fjjj8+v/8UXX6wsF2054YQTcjA8//zz009/+tMcYmof+C6KCG/RY3HuuefmHpoQYTUCzve///18MB7tvummm3KPXzGExnigr3/96/m273znO+miiy5K/+f//J98mWQ8ZoSsbbfdts7es5gXoXWvvfaqt21/+tOf8v/xmA0RB/wHHHBAWm655dKIESPy64lQGgFr+vTp873nESDivRw5cmReB+Nzj8eIdSpCR7zH0cYIP2+//fZ8zxfLv/LKKzlwxjLxmgYNGlTjPYrHi4AS62WsU/H5nn766fmzqy1CaLRpiy22yOtcrDd1ufLKK9Nxxx2XNtpoo7xcrDdxn8cee6yyzEsvvZS23377HDJPOeWUdNppp+XXEGGruFzZsccem5c944wzcsiNdTReH8ASUwKgTmPGjImjydITTzxRuvjii0srrrhi6T//+U++bf/99y994xvfyD+vtdZapd12261yvzvuuCPf7+yzz67xePvtt1+pRYsWpTfeeCP//uyzz+bljjnmmBrLffe7383zzzjjjMq8IUOGlFZfffXSBx98UGPZAw88sNSpU6dKu95+++1832j7wkydOrXUqlWr0pVXXlmZt80225T22muvGstdffXV+TEvuOCC+R5j3rx5+f/7778/L3PcccfVu8yC2lb79cbPMe+ggw6ab9nyay268cYb8/IPP/xwZd4hhxxSatmyZf786mvT5Zdfnu/3yiuvVG6bO3duaZVVVikNHjy4tCBbbrllfu8bIh5ztdVWK22yySalTz/9tDL/rrvuys9/+umnV+bF88a8c889tzLvo48+KrVv3z6vPzfddFNl/quvvjrfe1deb7faaqv8vGUjR47M8//4xz8u8L088sgjS8svv3xp9uzZlXk77rhjvu9ll1023/JxW0xlsf5svPHGC3w/Bg0aVGrTpk3pzTffrMybPHly/hvbYYcd5nst/fv3r3xm4cQTTywtt9xypenTpy/weQAWl54kgAaIHoC4LO2uu+7KvS3xf32X2kW1uOgtiLPpRXH5XeSB6IUqLxdqLxe9MUVxn7i0a4899sg/f/DBB5VpwIABuUfn6aefXuTXFD0sLVu2TPvuu2+NSwujfcXLsuK549KwOJtfW1wKVV4mfo4z/fUtsziOOuqo+ebFpWrFywDjfYhetlB+H+LSvzvuuCO/Z9HrUl+b4nON3q5ib9K9996bHzMuhVtYVbvoyWmIuORw2rRp6ZhjjsnPVxa9hr17907/+7//O999opexbKWVVsq9kXGpWbS5LObFbdGzVlsUkYieybLogWnVqlVlvav9XsZ6Ha87eniip+3VV1+t8XhxOV703i1MtCcuGX3iiSfqvD16yaLYQ/RqRW9eWVzCGX9TUUky3tvar6W4HkUb43HeeeedhbYHYHEISQANEGMhoopZjKuIS6TiAG2//farc9k4cIsxHrUPoDfccMPK7eX/I6TEOJCiOPAtirEXcTnWFVdckdtRnMoHrXEAvqiuv/76fDlaXEb1xhtv5CmKN8R4nRg7UxbjjqJNcYBdn1gmXnPnzp1TY4oxRLV9+OGH+XK/GJsVB/nxPpSXi8BYfs/iQDvG0SzsgD6CVHG8TASmGAMU434WJMbSRLBoiPJnXvuzDRGSah/sl8d0FXXq1Cmtueaa84XOmF97rFFYf/31a/wel9VFEImxYcXL3mJ8TzxGvJ54znI4LL+XZfGeNKRAQ5Q+j+eKdSvaEMUsYkxRWXw2EcLqei/ibyQC7qRJk2rM79mzZ43fY5xTqOt1AzQGJcABGijOcsc4kilTpuSxGXGA/WUof3dRHLwOHjy4zmU222yzRXrM119/vXKmv/bBdDkoxNn7xlRfj1LtIgFFxZ6OsuhJicIKMcYrxrrEAXm8RzFWZ3G+5ynG60QojMeMohMx1ih6fCLALkiEmyg+EAf0MXaoMUVP5KLMr10QpCEieO+44445HJ111lk5rEc4i964CDq138u6Pou6RNCZMGFC7m295557ci/jJZdcksc6xfikxdGYrxugIYQkgAaKM+5RLSwKEdx88831LhcVx6LAQvQyFHuTypcvxe3l/+NAtNxTUxYHmEXlyncRJhrrO4wiBMWlWL/73e/mOwCNy52iyEFUV4sz+HHwHIPpP/vssxqXbxXFMnGZWvTy1NebVD77X7tIwaJcMhU9B+PGjcsH23HQXQx9td+zOPgvFpaoT4SrWD7ekz59+uRejoYUY4geqCjmET1yw4YNW+Cy5c88PtvaPVQxr3x7Y4r3pFhc4ZNPPknvv/9+LnARomx59CJGz2hUrCurqwjEoorLAqNYRkzRMxmVEKPSX7xP8V5H1bva63n5byTCaWOHToBF5XI7gAaKHotLL700VwuLA+T6xEFoBJqLL764xvyodhe9KdELFcr/RyApql2tLkJMjBuKM/J1HfQvTinkCAQxriMOYuOyweIUPTShXM0vnjvGqtR+PcUz+bFM/FxXT0F5mQgtMbYpSlMXRS9DQ5UDXe0ehNrvWRxox5iXqIJWLkFeV5tCXEYYY7FuueWWXO0tepMa0jMX71UsGwf/48ePn+/2CMlRHjzEuKjVVlstXXbZZbnse1mM/4oKdHVVNPyi4vLMCLZlse7GdzuV17u63ssINIvyedSldin2uEQvKt3F80R74nmjmmOUHy9e+hcVHMtf2hzrCkA16UkCWAT1Xe5WFAEqzuDHAXIcBG6++eZ5oHocFEZRhvIYpLhULA7O46A0xn9ss802uZckxgbVFuXBH3jggdzTEZf8xUFn9NrEpVHRaxU/N1T0CsVz1FdCOcaefPWrX81BKi67isvRrrvuulwm+vHHH8/hKr4HKp43LkuLMtnxeqP3JQJf9GCUL33729/+lm8rP1cUI4jXEv9HcIjA9NprrzW47XHwHL0eURY7DrijrfHe1tX7EWXD47a4pCwuHYzLwKInJS6ti96y4uWS8Rqj7fEeR2nthoheteiFid69aFNcBhglxWN+jPWJA/7oPYsQFfPicWMMWbQnPvcIBVF2O8p7x3dlNbYIPPHdQ9Gu6LWJ9SwCSJSnD7G+RftinY7iIRHgo2fxi17CFgEoSsrHexHjxiIERsCOIFjuWT377LNzKfdoT6xDEVQvv/zyHCDjswWousWuiwewDJUAX5DaJcDDxx9/nMsUd+/evdS6devS+uuvX/rlL39Zo4xxiHLQUTa7S5cupQ4dOpT22GOP0qRJk+Yr61wu2T106NBSjx498mN269attMsuu5SuuOKKyjINKQF+7LHH5mWK5ZdrO/PMM/Myzz33XKVU9M9//vNSr169Ks8dJc2Lj/Hf//43v8bevXvn8s6rrrpqaeDAgaWnnnqqskw8TpQzj9LZUe75gAMOKE2bNq3eEuD/+te/5mvbu+++W9p7771LK620Un6cKMce5aPres/eeeedXAo82tK2bdvSOuusk9/DOXPmzPe4UbY6SobH4y+KKM8dJbw33XTTXDq7Xbt2udT3sGHDSu+//36NZW+++eZcOjza0rlz59LBBx883/NFCfBYF2qLMtt1ldauvf6V19uHHnqodMQRR5RWXnnl0gorrJCf69///neN+/79738v9e3bN5cXj3X1lFNOKd177735/g888MBCn7uuEuBRVj3KeMc6Ha9z3XXXLZ188smlGTNm1Ljf008/XRowYEBuW7xvUVL/H//4R4P+BqNttdsI0JhaxD/VDmoAUG1R2S/GU0Vv3tIsLhmMHqsozFFX+XMAFs6YJACWeTFu6dlnn82X3QGAMUkALLOiEMZTTz2Vfv3rX+fvEIpCFgCgJwmAZdbvf//7fGlaFIGIan7xPUEAYEwSAABAgZ4kAACAAiEJAABgWSrcEF9mOHny5PwFdvFFeQAAwLKpVCqljz/+OHXv3j21bNly2Q1JEZB69OhR7WYAAABNxKRJk9Kaa6657Iak6EEqvxEdO3asdnMAAIAqmTlzZu5AKWeEZTYklS+xi4AkJAEAAC0WMgxH4QYAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKGhV/IUlr0WLareAJaFUqnYLAABoLHqSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgIJWxV+ApcjYFtVuAY3tu6VqtwCg6oa3GF7tJrAEnFE6Iy1N9CQBAAAUCEkAAAAFQhIAAECBkAQAAFAgJAEAABQISQAAAAVCEgAAQIGQBAAAUCAkAQAAFAhJAAAATSUkff755+m0005LvXr1Su3bt0/rrrtu+sUvfpFKpVJlmfj59NNPT6uvvnpepn///un111+vZrMBAIBmrKoh6fzzz0+XXnppuvjii9Mrr7ySfx85cmT67W9/W1kmfr/ooovSZZddlh577LHUoUOHNGDAgDR79uxqNh0AAGimWlXzyf/xj3+kvfbaK+22227597XXXjvdeOON6fHHH6/0Il144YXp1FNPzcuF6667LnXt2jXdcccd6cADD6xm8wEAgGaoqj1J22yzTRo3blx67bXX8u/PPfdceuSRR9LAgQPz72+//XaaMmVKvsSurFOnTqlPnz5p/PjxdT7mnDlz0syZM2tMAAAAS0VP0k9/+tMcYnr37p2WW265PEbpnHPOSQcffHC+PQJSiJ6jovi9fFttI0aMSMOHD/8SWg8AADRHVe1JuuWWW9INN9yQxo4dm55++ul07bXXpl/96lf5/8U1bNiwNGPGjMo0adKkRm0zAADQvFW1J+nkk0/OvUnlsUWbbrppeuedd3Jv0ODBg1O3bt3y/KlTp+bqdmXx+xZbbFHnY7Zt2zZPAAAAS11P0n/+85/UsmXNJsRld/Pmzcs/R2nwCEoxbqksLs+LKnf9+vX70tsLAAA0f1XtSdpjjz3yGKSePXumjTfeOD3zzDPpggsuSIcddli+vUWLFumEE05IZ599dlp//fVzaIrvVerevXsaNGhQNZsOAAA0U1UNSfF9SBF6jjnmmDRt2rQcfo488sj85bFlp5xySpo1a1Y64ogj0vTp09N2222X7rnnntSuXbtqNh0AAGimWpTiy4iasbg8L8qGRxGHjh07Vrs5qUWLareAJaEqf0VjrUzNzneb9eYYoEGGt1CluDk6o3RGWpqyQVXHJAEAADQ1QhIAAECBkAQAAFAgJAEAADSV6nYAVF+L4YqANDelMxQBAfgi9CQBAAAUCEkAAAAFQhIAAECBkAQAAFAgJAEAABQISQAAAAVCEgAAQIGQBAAAUCAkAQAAFLQq/gIAsFhatKh2C1gSSqVqtwCqQk8SAABAgZAEAABQICQBAAAUCEkAAAAFQhIAAECBkAQAAFAgJAEAABQISQAAAAVCEgAAQIGQBAAAUCAkAQAAFAhJAAAABUISAABAgZAEAABQICQBAAAUCEkAAAAFQhIAAECBkAQAAFAgJAEAABQISQAAAAVCEgAAQIGQBAAAUCAkAQAAFAhJAAAABUISAABAUwlJa6+9dmrRosV809ChQ/Pts2fPzj936dIlrbDCCmnfffdNU6dOrWaTAQCAZq6qIemJJ55I77//fmW677778vz9998//3/iiSemO++8M916663poYceSpMnT0777LNPNZsMAAA0c62q+eSrrrpqjd/PO++8tO6666Ydd9wxzZgxI1111VVp7Nixaeedd863jxkzJm244Ybp0UcfTX379q1SqwEAgOasyYxJmjt3brr++uvTYYcdli+5e+qpp9Jnn32W+vfvX1mmd+/eqWfPnmn8+PFVbSsAANB8VbUnqeiOO+5I06dPT4ceemj+fcqUKalNmzZppZVWqrFc165d8231mTNnTp7KZs6cuQRbDQAANDdNpicpLq0bOHBg6t69+xd6nBEjRqROnTpVph49ejRaGwEAgOavSYSkd955J/31r39NP/jBDyrzunXrli/Bi96loqhuF7fVZ9iwYXk8U3maNGnSEm07AADQvDSJkBQFGVZbbbW02267VeZttdVWqXXr1mncuHGVeRMmTEgTJ05M/fr1q/ex2rZtmzp27FhjAgAAWGrGJM2bNy+HpMGDB6dWrf5fc+JSuSFDhqSTTjopde7cOYedY489Ngckle0AAIBmG5LiMrvoHYqqdrWNGjUqtWzZMn+JbBRjGDBgQLrkkkuq0k4AAGDZUPWQtOuuu6ZSqVTnbe3atUujR4/OEwAAwDIzJgkAAKCpEJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAAAKhCQAAIACIQkAAKBASAIAAGhKIem9995L3/ve91KXLl1S+/bt06abbpqefPLJyu2lUimdfvrpafXVV8+39+/fP73++utVbTMAANB8VTUkffTRR2nbbbdNrVu3TnfffXd6+eWX069//eu08sorV5YZOXJkuuiii9Jll12WHnvssdShQ4c0YMCANHv27Go2HQAAaKZaVfPJzz///NSjR480ZsyYyrxevXrV6EW68MIL06mnnpr22muvPO+6665LXbt2TXfccUc68MADq9JuAACg+apqT9Kf/vSntPXWW6f9998/rbbaamnLLbdMV155ZeX2t99+O02ZMiVfYlfWqVOn1KdPnzR+/PgqtRoAAGjOqhqS3nrrrXTppZem9ddfP917773p6KOPTscdd1y69tpr8+0RkEL0HBXF7+XbapszZ06aOXNmjQkAAGCpuNxu3rx5uSfp3HPPzb9HT9KLL76Yxx8NHjx4sR5zxIgRafjw4Y3cUgAAYFlR1Z6kqFi30UYb1Zi34YYbpokTJ+afu3Xrlv+fOnVqjWXi9/JttQ0bNizNmDGjMk2aNGmJtR8AAGh+qhqSorLdhAkTasx77bXX0lprrVUp4hBhaNy4cZXb4/K5qHLXr1+/Oh+zbdu2qWPHjjUmAACApeJyuxNPPDFts802+XK7Aw44ID3++OPpiiuuyFNo0aJFOuGEE9LZZ5+dxy1FaDrttNNS9+7d06BBg6rZdAAAoJmqakj62te+lm6//fZ8idxZZ52VQ1CU/D744IMry5xyyilp1qxZ6YgjjkjTp09P2223XbrnnntSu3btqtl0AACgmapqSAq77757nuoTvUkRoGICAABo1mOSAAAAmhohCQAAoEBIAgAAKBCSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAACoQkAACAphKSzjzzzNSiRYsaU+/evSu3z549Ow0dOjR16dIlrbDCCmnfffdNU6dOrWaTAQCAZq7qPUkbb7xxev/99yvTI488UrntxBNPTHfeeWe69dZb00MPPZQmT56c9tlnn6q2FwAAaN5aVb0BrVqlbt26zTd/xowZ6aqrrkpjx45NO++8c543ZsyYtOGGG6ZHH3009e3btwqtBQAAmrtF7klae+2101lnnZUmTpzYKA14/fXXU/fu3dM666yTDj744MrjPvXUU+mzzz5L/fv3rywbl+L17NkzjR8/vt7HmzNnTpo5c2aNCQAAYImFpBNOOCHddtttOdR885vfTDfddFMOJoujT58+6Zprrkn33HNPuvTSS9Pbb7+dtt9++/Txxx+nKVOmpDZt2qSVVlqpxn26du2ab6vPiBEjUqdOnSpTjx49FqttAADAsmmxQtKzzz6bHn/88Xzp27HHHptWX3319MMf/jA9/fTTi/RYAwcOTPvvv3/abLPN0oABA9Kf//znNH369HTLLbekxTVs2LB8qV55mjRp0mI/FgAAsOxZ7MINX/3qV9NFF12UiymcccYZ6X/+53/S1772tbTFFlukq6++OpVKpUV+zOg1+spXvpLeeOONPE5p7ty5OTQVRXW7usYwlbVt2zZ17NixxgQAALDEQ1KMF4oenz333DP96Ec/SltvvXUOSlGm+2c/+1keX7SoPvnkk/Tmm2/mnqmtttoqtW7dOo0bN65y+4QJE/KYpX79+i1uswEAABq3ul1cUhdV5m688cbUsmXLdMghh6RRo0bV+H6jvffeO/cqLcyPf/zjtMcee6S11lqr0iO13HLLpYMOOiiPJxoyZEg66aSTUufOnXOPUFzaFwFJZTsAAKDJhKQIP1GwIQotDBo0KPf21NarV6904IEHLvSx3n333RyI/v3vf6dVV101bbfddrm8d/wcInxFEIveqSgOEeOWLrnkkkVtMgAAwJILSW+99Vbu+VmQDh065N6mhYnKeAvSrl27NHr06DwBAAA0yTFJ06ZNS4899th882Pek08+2VjtAgAAWDpC0tChQ+ssq/3ee+/l2wAAAJapkPTyyy/n8t+1bbnllvk2AACAZSokxfcQxXcV1fb++++nVq0WeYgTAADA0h2Sdt111zRs2LA0Y8aMyrz4wtf4bqSoegcAALA0W+Sun1/96ldphx12yBXu4hK78Oyzz6auXbum3/3ud0uijQAAAE03JK2xxhrp+eefTzfccEN67rnnUvv27dP3v//9/H1HdX1nEgAAwNJksQYRxfcgHXHEEY3fGgAAgCpb7EoLUclu4sSJae7cuTXm77nnno3RLgAAgKUjJL311ltp7733Ti+88EJq0aJFKpVKeX78HD7//PPGbyUAAEBTrW53/PHHp169eqVp06al5ZdfPr300kvp4YcfTltvvXV68MEHl0wrAQAAmmpP0vjx49P999+fVlllldSyZcs8bbfddmnEiBHpuOOOS88888ySaSkAAEBT7EmKy+lWXHHF/HMEpcmTJ+efoyT4hAkTGr+FAAAATbknaZNNNsmlv+OSuz59+qSRI0emNm3apCuuuCKts846S6aVAAAATTUknXrqqWnWrFn557POOivtvvvuafvtt09dunRJN99885JoIwAAQNMNSQMGDKj8vN5666VXX301ffjhh2nllVeuVLgDAABYJsYkffbZZ6lVq1bpxRdfrDG/c+fOAhIAALDshaTWrVunnj17+i4kAACg2Vrk6nY///nP089+9rN8iR0AAEBa1sckXXzxxemNN95I3bt3z2W/O3ToUOP2p59+ujHbBwAA0LRD0qBBg5ZMSwAAAJbGkHTGGWcsmZYAAAAsjWOSAAAAmrNF7klq2bLlAst9q3wHAAAsUyHp9ttvn++7k5555pl07bXXpuHDhzdm2wAAAJp+SNprr73mm7fffvuljTfeON18881pyJAhjdU2AACApXdMUt++fdO4ceMa6+EAAACW3pD06aefposuuiitscYajfFwAAAAS8/ldiuvvHKNwg2lUil9/PHHafnll0/XX399Y7cPAACgaYekUaNG1QhJUe1u1VVXTX369MkBCgAAYJkKSYceeuiSaQkAAMDSOCZpzJgx6dZbb51vfsyLMuAAAADLVEgaMWJEWmWVVeabv9pqq6Vzzz23sdoFAACwdISkiRMnpl69es03f6211sq3AQAALFMhKXqMnn/++fnmP/fcc6lLly6N1S4AAIClIyQddNBB6bjjjksPPPBA+vzzz/N0//33p+OPPz4deOCBS6aVAAAATbW63S9+8Yv0z3/+M+2yyy6pVav/e/d58+alQw45xJgkAABg2QtJbdq0STfffHM6++yz07PPPpvat2+fNt100zwmCQAAYJkLSWXrr79+ngAAAJbpMUn77rtvOv/88+ebP3LkyLT//vs3VrsAAACWjpD08MMPp29/+9vzzR84cGC+DQAAYJkKSZ988kkel1Rb69at08yZMxe7Ieedd15q0aJFOuGEEyrzZs+enYYOHZpLi6+wwgq5F2vq1KmL/RwAAACNHpKiSEMUbqjtpptuShtttFFaHE888US6/PLL02abbVZj/oknnpjuvPPOdOutt6aHHnooTZ48Oe2zzz6L9RwAAABLpHDDaaedloPKm2++mXbeeec8b9y4cWns2LHp97//fVqcnqmDDz44XXnllbliXtmMGTPSVVddlR+3/DxjxoxJG264YXr00UdT3759F/m5AAAAGr0naY899kh33HFHeuONN9IxxxyTfvSjH6X33nsvf6Hseuutt6gPly+n22233VL//v1rzH/qqafSZ599VmN+7969U8+ePdP48ePrfbw5c+bky/6KEwAAwBItAR6hJqYQIeTGG29MP/7xj3Ow+fzzzxv8OHGJ3tNPP50vt6ttypQpeezTSiutVGN+165d8231GTFiRBo+fPgivR4AAIDF7kkqi0p2gwcPTt27d0+//vWv8yVxcRlcQ02aNCkdf/zx6YYbbkjt2rVLjWXYsGH5Ur3yFM8DAACwRHqSogfnmmuuyWOFogfpgAMOyJe3xeV3i1q0IXqdpk2blr761a9W5kUvVISviy++ON17771p7ty5afr06TV6k6K6Xbdu3ep93LZt2+YJAABgifYkxVikDTbYID3//PPpwgsvzJXmfvvb36bFtcsuu6QXXnghPfvss5Vp6623zkUcyj9HWfEoClE2YcKENHHixNSvX7/Ffl4AAIBG6Um6++6703HHHZeOPvrotP7666cvasUVV0ybbLJJjXkdOnTI34lUnj9kyJB00kknpc6dO6eOHTumY489Ngckle0AAICq9yQ98sgj6eOPP05bbbVV6tOnT74k7oMPPkhL0qhRo9Luu++ev0R2hx12yJfZ3XbbbUv0OQEAgGVbg0NS9N7Edxm9//776cgjj8yV6aJow7x589J9992XA9QX9eCDD+ZL+cqioMPo0aPThx9+mGbNmpUD0oLGIwEAAHzp1e3ikrjDDjss9yzFmKL4nqTzzjsvrbbaamnPPff8wg0CAABYKkuAhyjkMHLkyPTuu+/m70oCAABYpkNS2XLLLZcGDRqU/vSnPzXGwwEAACzdIQkAAKC5EJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAACaSki69NJL02abbZY6duyYp379+qW77767cvvs2bPT0KFDU5cuXdIKK6yQ9t133zR16tRqNhkAAGjmqhqS1lxzzXTeeeelp556Kj355JNp5513TnvttVd66aWX8u0nnnhiuvPOO9Ott96aHnrooTR58uS0zz77VLPJAABAM9eqmk++xx571Pj9nHPOyb1Ljz76aA5QV111VRo7dmwOT2HMmDFpww03zLf37du3Sq0GAACasyYzJunzzz9PN910U5o1a1a+7C56lz777LPUv3//yjK9e/dOPXv2TOPHj6/3cebMmZNmzpxZYwIAAFhqQtILL7yQxxu1bds2HXXUUen2229PG220UZoyZUpq06ZNWmmllWos37Vr13xbfUaMGJE6depUmXr06PElvAoAAKC5qHpI2mCDDdKzzz6bHnvssXT00UenwYMHp5dffnmxH2/YsGFpxowZlWnSpEmN2l4AAKB5q+qYpBC9Reutt17+eauttkpPPPFE+s1vfpO+853vpLlz56bp06fX6E2K6nbdunWr9/GiRyomAACApbInqbZ58+blcUURmFq3bp3GjRtXuW3ChAlp4sSJecwSAABAs+tJikvjBg4cmIsxfPzxx7mS3YMPPpjuvffePJ5oyJAh6aSTTkqdO3fO36N07LHH5oCksh0AANAsQ9K0adPSIYcckt5///0ciuKLZSMgffOb38y3jxo1KrVs2TJ/iWz0Lg0YMCBdcskl1WwyAADQzFU1JMX3IC1Iu3bt0ujRo/MEAACwTI5JAgAAqCYhCQAAoEBIAgAAKBCSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAAmkpIGjFiRPra176WVlxxxbTaaqulQYMGpQkTJtRYZvbs2Wno0KGpS5cuaYUVVkj77rtvmjp1atXaDAAANG9VDUkPPfRQDkCPPvpouu+++9Jnn32Wdt111zRr1qzKMieeeGK6884706233pqXnzx5ctpnn32q2WwAAKAZa1XNJ7/nnntq/H7NNdfkHqWnnnoq7bDDDmnGjBnpqquuSmPHjk0777xzXmbMmDFpww03zMGqb9++VWo5AADQXDWpMUkRikLnzp3z/xGWonepf//+lWV69+6devbsmcaPH1+1dgIAAM1XVXuSiubNm5dOOOGEtO2226ZNNtkkz5syZUpq06ZNWmmllWos27Vr13xbXebMmZOnspkzZy7hlgMAAM1Jk+lJirFJL774Yrrpppu+cDGITp06VaYePXo0WhsBAIDmr0mEpB/+8IfprrvuSg888EBac801K/O7deuW5s6dm6ZPn15j+ahuF7fVZdiwYfmyvfI0adKkJd5+AACg+ahqSCqVSjkg3X777en+++9PvXr1qnH7VlttlVq3bp3GjRtXmRclwidOnJj69etX52O2bds2dezYscYEAACwVIxJikvsonLdH//4x/xdSeVxRnGZXPv27fP/Q4YMSSeddFIu5hCB59hjj80BSWU7AACg2YWkSy+9NP+/00471ZgfZb4PPfTQ/POoUaNSy5Yt85fIRkGGAQMGpEsuuaQq7QUAAJq/VtW+3G5h2rVrl0aPHp0nAACAZaJwAwAAQFMhJAEAABQISQAAAAVCEgAAQIGQBAAAUCAkAQAAFAhJAAAABUISAABAgZAEAABQICQBAAAUCEkAAAAFQhIAAECBkAQAAFAgJAEAABQISQAAAAVCEgAAQIGQBAAAUCAkAQAAFAhJAAAABUISAABAgZAEAABQICQBAAAUCEkAAAAFQhIAAECBkAQAAFAgJAEAABQISQAAAAVCEgAAQIGQBAAAUCAkAQAAFAhJAAAABUISAABAgZAEAABQICQBAAAUCEkAAAAFQhIAAECBkAQAAFAgJAEAABQISQAAAAVCEgAAQIGQBAAA0FRC0sMPP5z22GOP1L1799SiRYt0xx131Li9VCql008/Pa2++uqpffv2qX///un111+vWnsBAIDmr6ohadasWWnzzTdPo0ePrvP2kSNHposuuihddtll6bHHHksdOnRIAwYMSLNnz/7S2woAACwbWlXzyQcOHJinukQv0oUXXphOPfXUtNdee+V51113XeratWvucTrwwAO/5NYCAADLgiY7Juntt99OU6ZMyZfYlXXq1Cn16dMnjR8/vqptAwAAmq+q9iQtSASkED1HRfF7+ba6zJkzJ09lM2fOXIKtBAAAmpsm25O0uEaMGJF7nMpTjx49qt0kAABgKdJkQ1K3bt3y/1OnTq0xP34v31aXYcOGpRkzZlSmSZMmLfG2AgAAzUeTDUm9evXKYWjcuHE1Lp2LKnf9+vWr935t27ZNHTt2rDEBAAAsFWOSPvnkk/TGG2/UKNbw7LPPps6dO6eePXumE044IZ199tlp/fXXz6HptNNOy9+pNGjQoGo2GwAAaMaqGpKefPLJ9I1vfKPy+0knnZT/Hzx4cLrmmmvSKaeckr9L6YgjjkjTp09P2223XbrnnntSu3btqthqAACgOatqSNppp53y9yHVp0WLFumss87KEwAAwDI9JgkAAKAahCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAAqEJAAAgAIhCQAAoEBIAgAAKBCSAAAACoQkAACAAiEJAACgQEgCAAAoEJIAAAAKhCQAAIACIQkAAKBASAIAACgQkgAAAJa2kDR69Oi09tprp3bt2qU+ffqkxx9/vNpNAgAAmqkmH5JuvvnmdNJJJ6UzzjgjPf3002nzzTdPAwYMSNOmTat20wAAgGaoyYekCy64IB1++OHp+9//ftpoo43SZZddlpZffvl09dVXV7tpAABAM9QqNWFz585NTz31VBo2bFhlXsuWLVP//v3T+PHj67zPnDlz8lQ2Y8aM/P/MmTO/hBazrKrK6vWfKjwnS1a1tlOzq/O0LDn2eTSaKqxLs22UmqWZTWS7VG5HqVRaekPSBx98kD7//PPUtWvXGvPj91dffbXO+4wYMSINHz58vvk9evRYYu2ETp2q3QKahcOtSDSOTudZl2gkdnA0kvM6nZeako8//jh1WsD63aRD0uKIXqcYw1Q2b9689OGHH6YuXbqkFi1aVLVty5JI6RFMJ02alDp27Fjt5rAUsy7RGKxHNBbrEo3FulQd0YMUAal79+4LXK5Jh6RVVlklLbfccmnq1Kk15sfv3bp1q/M+bdu2zVPRSiuttETbSf3ij94fPo3BukRjsB7RWKxLNBbr0pdvQT1IS0XhhjZt2qStttoqjRs3rkbPUPzer1+/qrYNAABonpp0T1KIS+cGDx6ctt566/T1r389XXjhhWnWrFm52h0AAMAyF5K+853vpH/961/p9NNPT1OmTElbbLFFuueee+Yr5kDTEpc8xndb1b70ERaVdYnGYD2isViXaCzWpaatRWlh9e8AAACWIU16TBIAAMCXTUgCAAAoEJIAAAAKhCQAlioPPvhg/nLw6dOn17vMNddcU+M78s4888xc+Acak/Wq6YuiX9/85jdThw4dKtuEuubFNuWOO+5o0GP63JcNQtJSJv6wjz322LTOOuvkaijxTc177LFHje+SWlz//Oc/80bi2WefTU3lm6h//vOfp969e6d27drlLxDu379/uu222/K3JYeddtopt/mmm26qcd8oFb/22mvXOGCK5b71rW/VWC4OsmJ+HHRRv0MPPTS/TzG1bt069erVK51yyilp9uzZjfL48bjxGb/zzjs15g8aNCg/9xc9eI4dWrn95SnWq6J4LUOHDk1dunRJK6ywQtp3333n+yJrvvg6dNRRR813W7zvcduifNYNqYz62muvNen1kvrF+xvvc1NS10H0j3/840bZ/5b94Q9/SMstt1x677336rx9/fXXz1+NUt6uxXYsDvRXXnnlvH987LHH0rK8fypO5f39qFGj0vvvv5+PbcrbhLrmxe8DBw5s0HM29ude14mdsoYe5yzuOlw+PipPsf+L7yi97bbbaiwXx11RaXr11VdP7du3z+vb66+/npozIWkpEiEmVtz7778//fKXv0wvvPBCLof+jW98Ix9kNCdxMLHNNtuk6667Lg0bNiw9/fTT6eGHH84HPnFwPmPGjMqycRBz6qmnps8++2yBj9mqVav017/+NT3wwANfwitofmKHEzuRt956K+9gLr/88ly6tLHExjk2wEvKxhtvnNtfnh555JEat5944onpzjvvTLfeemt66KGH0uTJk9M+++yzxNqzLIqTOrGj//TTT2uE07Fjx6aePXs26nPFTny11VZr8uslS7c4oIwTK41lzz33zI937bXXzndb7APfeOONNGTIkPz7V77ylXTxxRfnY4HYnsUB86677pq/NmVZ3T8VpxtvvDHf9uabb+ZjpwiY5W1CXfPiRGxDS3E39ue+MA09zllcHTt2rLxvzzzzTBowYEA64IAD0oQJEyrLjBw5Ml100UXpsssuy2E8wnks11gnS5ukKAHO0mHgwIGlNdZYo/TJJ5/Md9tHH31Uevvtt6N7pfTMM8/UmB/zHnjggfz7hx9+WPrud79bWmWVVUrt2rUrrbfeeqWrr7463xbLFacdd9wxz//8889Lw4cPz8/dpk2b0uabb166++67K89Rft6bb765tN122+XH3XrrrUsTJkwoPf7446Wtttqq1KFDh9K3vvWt0rRp0xr0Wo8++uh8n/fee2++2z7++OPSZ599ln+ONn7/+98vdenSpTR69OjKMqNGjSqttdZald/HjBlT6tSpU+nwww8vff3rX6/3/aFugwcPLu2111415u2zzz6lLbfcsrKOnHvuuaW11147f/6bbbZZ6dZbb60su6D1LsRn8OMf/7jUsmXL0gsvvFCZH88Zz122oOcpr4fFqXzfM844I6+39Zk+fXqpdevWNdr8yiuv5McYP378F3z3KK5Dm2yySen666+vzL/hhhvy51j8rGfPnl069thjS6uuumqpbdu2pW233TZvS8ri7zU+m7vuuqu06aab5mX69OlTY90p/82X1bUOXHnllaXevXvn+2+wwQY1tiFfxnpJw7Y5sZ2P9eHkk08urbzyyqWuXbvmz7MotuVHHHFEabXVVsuf58Ybb1y68847K7f/7W9/q+yf1lxzzfx4xX1p7C/OOuus0oEHHlhafvnlS927dy9dfPHFNW4vfobl/Uvt9aqh+8s//OEPpZ122qnUvn37vL784x//qCxz0kknldZff/0635NYz+szY8aM/Nh//etfS8v6/qm+zy2WrWteiJ9vv/32yn0nTZqU14dY52KdiGOZRx99dLG2Jwv73MvbtOJUXscbepwT7rjjjrxfjjb06tWrdOaZZ1aOl+pbh2tvK8vrcevWrUu33HJL/n3evHmlbt26lX75y1/W2G/G89x4442l5kpIWkr8+9//LrVo0SLviOvTkJA0dOjQ0hZbbFF64okn8vL33Xdf6U9/+lO+LQ5CyhvY999/Pz9nuOCCC0odO3bMfwivvvpq6ZRTTsl/PK+99lqN542Nwz333FN6+eWXS3379s0blNgYPPLII6Wnn346HxgfddRRC32t8ccZG6XY4S1MbDyOP/743MbYcZZ3evWFpAhdsXEqH8AISYu3E4oDxthglnfYZ599duXzf/PNN/P7HRvPBx98cKHrXXHntOeee5Z22223eg9GF/Q8//3vf/MOKB4rAnqsw7ERD7GziZ3c6quvnnccEdjeeeedyuOOGzcu3y/Wh6KePXvmdYvGW4fi/dxll10q8+Pn+HstftbHHXdcPkj985//XHrppZfy/NgmlLdJ5QOKDTfcsPSXv/yl9Pzzz5d23333HFLmzp3boJAUQS3Wh1hn3nrrrfx/586dS9dcc82Xtl7S8JAU+6A44Iv9zrXXXpv3h/HZl/cZsc+JYBTz4jOIgBTrT3jjjTfySbdYz+L+f//73/OB5KGHHlp5vthfrLjiiqURI0bkz+miiy4qLbfccpXniBN88RnGZxufYfmEX+31alH2lxHy47n222+//Pzlg9lY52OZhx56qMbJwXgNV1xxRZ3v15w5c/IBbKzz//rXv0rLkgWFpPic4gTtAQccUPnbq2te7ZAU7/c666xT2n777XPAfv311/OJ4HKoWdTtycI+9/j8LrzwwrzuRJtiijYsynHOww8/nO8fzxl/A7HuxjYx/m4WtA7X3lbGNitOYrZu3Tr/7YR4vNrHl2GHHXbI2+vmSkhaSjz22GN5Bb3tttu+UEjaY4898hmJht4/xMHKOeecU2Pe1772tdIxxxxT437/8z//U7k9dhAxLw4+y2LnE2dXFmbq1Kn5vg05OC1vPOLMc/lM4IJCUvjpT39a+spXvpI3TEJSw3dCccAQO+k4+Iv3LM6u//73v8/vfQSQ4pnQMGTIkNJBBx200PWuuHOKg4N4ntjYh9q9Cwt7nvLBc+2wEwdLcUbsueeeywey/fr1ywFo5syZld6MOOtbW6zncZBD4x3IxI451qF//vOfeYoz+3FQV/6s4wAgds7xmZRF8Int0MiRI2t8zjfddFNlmQhQcQIkDmQaEpLWXXfd0tixY2u08Re/+EVeN76s9ZKGh6ToBar9t/mTn/wk/3zvvffm7VEceNYlPovaJ93iwDfu8+mnn+bfY38RB85F3/nOd/IVHGW1exrqWq8WZ39ZDkXRe10Woa8YxK+66qq8npW3WWURBmO7HKExnrvY47os7p+KU/lzqH1So755xc/38ssvz6G5fGKmtkXdnjTkc6+rR2dRjnPihFPtE+m/+93vcnir6zWWxfPG/PL7Fn8XsY0eM2ZMZZk4sRDLTJ48ucZ9999//xw2m6tW1b7cj4YpFyr4oo4++ug8ID3G+MS1yzEwNsb+LKh4QozN2HbbbWvMj9+fe+65GvM222yzys9du3bN/2+66aY15k2bNm2JvNa4jviss87KRS3iNS7IT37ykzye5uqrr87X3NIwMfbt0ksvTbNmzcpjkmKMV6xLL730UvrPf/6TKwUVzZ07N2255ZaLtN5ttNFG6ZBDDkk//elP09///vcat8W1+At7nvoUB+PGetqnT5+01lprpVtuuaVyfT9fjlVXXTXttttuebBw/K3Hz6usskrl9hgrENfdF7c5USzk61//enrllVdqPFa/fv0qP3fu3DltsMEG8y1Tl1iH43nisz/88MMr8//73/+mTp06fWnrJQ1X3L+EGDxe3p/E4Ps111wzj9GpS+yrnn/++XTDDTdU5sW6N2/evPT222+nDTfccL71qfx7DI5vqMXdX8ZrCfF6ygVlDjvssDxO8re//W1accUV8/5q//33zz/X3i7H6//ggw/SlVdemfdpMV6kMcbjLY37p6LYJiyueE/j77chj7Eo25OFfe5f5Dgn1rHYPp1zzjmVeZ9//nkeMxTbqOWXX77ex471KvbPIZaN8dtHHXVUHncVxcGWVULSUiIGF8YA4ldffbXeZVq2bDlfyKg9yC8OFqNS05///Od03333pV122SUXffjVr371hdsYBzJl0da65sVOqSEHUVHhZUGvtS7f+9738us4++yzF1jxJR47ikEMHz487b777ov0HMuyGKS53nrr5Z9jh7355punq666Km2yySZ53v/+7/+mNdZYo8Z9yoNgF2W9i88lDnZqV+D55JNPFvo8DRXrQDxHHOCWB+zGQW0UDClWF4rqdnEbjSsOAH/4wx/mn0ePHv2lP395XYqDygjMRVFZrC5fxnpJ/Yr7ktr7kyjSsSDxGR155JHpuOOOm++2xi4Y8kX2l8X944EHHphDUpzI2WGHHfLB74gRI+rdLsfUt2/ffKwQ2+XYxy2r+6fGsLB1anG3Jwv73L/IcU60I7ZTdRUcisIPCxLHj8X3L8LcX/7yl3T++efnkFTeD8Y+sRzuyr8351LoqtstJeJsRlQRiQOKOGtRWxzcRbgIUZ2krK5y3rHc4MGD0/XXX5/Pkl1xxRV5fps2bSpnHooVT7p37z7f2dP4Pc6uLgnxxxo7iDjrF2flaosNQZyhqet+sROJs0lRCXBB4kxMLP+b3/ymUdu+rIj37mc/+1muthPrQRwMTpw4sbKzLk9RzWxh611tcZ84gI7HL66LDXmeutbhusQ6FGf+yhv7qHIUO69iSdeo6hPPVfvsMo1TiSpCaZzEie1a0brrrps/x+I2J5Z74okn5tvmPProo5WfP/roo1zKt9wrsCDRqx3btajUWHtdivL21VovWTxxQPfuu+/WW/L9q1/9anr55Zfn+3xiKn82tden8u/F9Sm2EQv6DBtzfxln9qPnKE5IjRkzJgf07bfffqH3iwPuOXPmLNJzUfc6FcdPH3744RLZntQl1sWFbSMWdJwT63nst+paz8sn0Re2DtcOeJ/+/5VI43VEUCruI6PnNHotm/M+Uk/SUiQCUnTbx2Un0eUaf8QRFuLMfPzBxGUmcSbpvPPOyyt0dOHGQWxRlLKNA8Iohxwb0rvuuquyE4ju+Th7EmXF49KFOPMQXcUnn3xyLvUcBy9xxiA22LHxKF660Niiuzi+WyTOysTPW2+9df7j/tvf/pY3EHHAVNf3CcSlO3GfuJyufMlfXeK1xRmX5lY6/csUO/BYN+K9ju+MiLOesYPebrvtcon2ODCIg4YIRgta7+oSZ0HjrFxcChNl38sHDQt7nriELs7OxeN/+9vfzutzlGqN+8XZsLg9gnesz7EDOOigg/Jjx3oel0rE94/ECYl4vAjSsfGPvykaV7z35cviap9pjTPCcSlJrFvxWcSZ/ig9G5eA1L40MraDcTlI/K3Hd6rFZXsN/W6d+PuPnoX47CO0xXr55JNP5rBV/h6aJb1e0jh23HHH3NsSl/RecMEF+aAwrkQof1dOXGIdf8cRcn/wgx/kdSxCU+w7o4R2WXxmsa7FOhS3xdcBRA9hWZy5j4PE2A9HMI7vJqqtMfeXsb5HMIq/lXgNRXGyNPaNUTI8TvbE5XZxjBDfrxTb5mVN/P3G90gWxSXhxUt5F0XsG84999y8LsQxR7zHURo7wlBdoWBxtie1xfoVJ/BiHYsrNeLyuLoukavvOCf2s3F1TGwz99tvvxyM4hK8F198Mfc8LWgdjiuQyu9fBKNY/++9997K1x/E39IJJ5yQHyd6K+MY87TTTsvvR1P7PrNGVe1BUSyaGDQXlcJisF4MNI8yo1F5qVx4ICrLxUDBGMAc1cSiukmxMEEMJIyKUHF7VF6JwYtRiaVYwrJHjx554F6xBHhUR4nnigHV9ZU0LRZ8qGugcn2DEusTFWeiyEKUQo3XGlVd+vfvnwcdRjnK4oDGohhAXSxvWd9zRwWXjTbaSOGGL1A9KIpxRJnmGGwflXmiMEesIzFvwIABlepMC1vv6hpMGgNQa5dLjs99Qc8TYlBrVN6Lgczl+8YA7Bi8Wv6bid/LVXvKYgB3DK4ul3vde++9cwUglnwFqtoDqeOziBLNUTJ+QSXAY9B6VDSLzzVK+0dhjkUpAR7FIWI7GfePzz0qNRWL4yzp9ZKGF26ovZ2vPfA+BtiXyyRHMZAoNR9VxMpi/fnmN79ZWmGFFfLg9Ci/XCywEPuLKN0dA9Hj7z8+q9/85jc1njMqckaV1latWi2wBPii7i8XVEAo1qkoSlB7wHz8jcQ2Koo1xPob27c4FlhWCzfULp8dU7lQ1OIUbghRWGbffffNFeNinYivNokiWouzPWno5x4VgGMdrl0CvCHHOSEKE22zzTZ5Xxvtju1isSJiXetwuXBDeYptbhS3Ouecc/JxUnE7d9ppp+VjsVgmCkXUVyyluWgR/1Q7qAEAVEucYY8z5TEBBGOSAAAACoQkqiKux69vinFHAABQLS63oyrKpZfrEmV0F6X8JgAANCYhCQAAoMDldgAAAAVCEgAAQIGQBAAAUCAkAQAAFAhJAAAABUISAABAgZAEAABQICQBAACk/+f/AxQkRyAvQG1ZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Compare all models \n",
    "\n",
    "## Custom CNN vs ResNet50 vs MobileNet vs InceptionV3 vs EfficientNetB0 Accuracy Comparison\n",
    "model_names = ['Custom_CNN', 'ResNet50', 'MobileNet', 'InceptionV3', 'EfficientNetB0']\n",
    "accuracies = [85.00, 83.00, 78.00, 76.00, 83.00]   \n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_names, accuracies, color=['blue', 'orange', 'green', 'red', 'purple'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ad81b7",
   "metadata": {},
   "source": [
    "SAVE THE BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be8d6c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the best model - Custom CNN model\n",
    "torch.save(customcnn_model.state_dict(), 'brain_tumor_customCNN_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f83d5",
   "metadata": {},
   "source": [
    "STREAMLIT UI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
